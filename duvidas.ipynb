{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "964a1d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports\n",
    "import anthropic\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bae4523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cca866a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "Você é sênior em aprendizagem de máquina, especializado em visão computacional e python\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbb4b41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 4.0 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "def run_claude_stream(rules, system_prompt, model=\"claude-sonnet-4-20250514\"):\n",
    "    stream = client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=25000,\n",
    "        temperature=0.7,\n",
    "        system=system_prompt,\n",
    "        messages=rules,\n",
    "        stream=True\n",
    "    )\n",
    "    full_text = \"\"\n",
    "    \n",
    "    for event in stream:\n",
    "        if event.type == \"content_block_delta\":\n",
    "            print(event.delta.text, end=\"\", flush=True) \n",
    "            full_text += event.delta.text\n",
    "            \n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5fe00fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "def extract_text_from_pdf_fitz(pdf_path):\n",
    "    text = \"\"\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document[page_num]\n",
    "        text += page.get_text() + \"\\n\"\n",
    "    pdf_document.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23e07ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "artigo = f\"{extract_text_from_pdf_fitz(r\"C:\\Users\\Renanzin\\Reproducao AM\\Deteccao-de-lesoes-na-pele\\artigo_am.pdf\")}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70af612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Considere esse artigo:\n",
    "´´´\n",
    "{artigo}\n",
    "´´´\n",
    "\n",
    "Com base nele considere esses requisitos dessa atividade:\n",
    "´´´\n",
    "- O artigo escolhido deve focar apenas em Deep Learning. A replicação deve envolver o treinamento de modelos \n",
    "não-deep (shallow learning), para comparar com os resultados originais.\n",
    "Com base nessas explicações, considere essa estrutura do meu repositório para realizar essa replicação:\n",
    "\"\n",
    "└───datasets\n",
    "    ├───HAM10000\n",
    "    │   ├───HAM10000_images_part_1\n",
    "    │   ├───HAM10000_images_part_2\n",
    "    │   ├───HAM10000_segmentations_lesion_tschandl\n",
    "    │   │   ├───HAM10000_segmentations_lesion_tschandl\n",
    "    │   │   └───__MACOSX\n",
    "    │   │       └───HAM10000_segmentations_lesion_tschandl\n",
    "    │   └───ISIC2018_Task3_Test_Images\n",
    "    │       ├───ISIC2018_Task3_Test_Images\n",
    "    │       └───__MACOSX\n",
    "    │           └───ISIC2018_Task3_Test_Images\n",
    "    └───ISIC\n",
    "        ├───ISIC_2019_Test_Input\n",
    "        └───ISIC_2019_Training_Input\n",
    "\"\n",
    "Agora com base em tudo isso, gere o código da replicação do estudo. O repositório do artigo tem 3 arquivos .pynb\n",
    "que são muito grandes, mas se quiser posso passá-los, caso seja necessário.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d9d58a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_rules = [\n",
    "    {\"role\": \"user\", \"content\": f\"{prompt}\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dd61bf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Com base no artigo analisado, vou criar uma replicação focada em comparar métodos de **shallow learning** com os resultados de **deep learning** (ViT) apresentados no paper. O artigo propõe um framework de Federated Learning com Vision Transformers, mas nossa replicação será centralizada usando métodos tradicionais.\n",
      "\n",
      "## Estrutura da Replicação\n",
      "\n",
      "```python\n",
      "# requirements.txt\n",
      "numpy==1.24.3\n",
      "pandas==2.0.2\n",
      "scikit-learn==1.3.0\n",
      "opencv-python==4.8.0.74\n",
      "matplotlib==3.7.1\n",
      "seaborn==0.12.2\n",
      "pillow==10.0.0\n",
      "joblib==1.3.1\n",
      "tqdm==4.65.0\n",
      "```\n",
      "\n",
      "## 1. Configuração e Utilitários\n",
      "\n",
      "```python\n",
      "# utils.py\n",
      "import os\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import cv2\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
      "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
      "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_auc_score\n",
      "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "import joblib\n",
      "from tqdm import tqdm\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "\n",
      "class Config:\n",
      "    \"\"\"Configurações globais do projeto\"\"\"\n",
      "    # Caminhos dos datasets\n",
      "    HAM10000_PATH = \"datasets/HAM10000\"\n",
      "    ISIC2019_PATH = \"datasets/ISIC\"\n",
      "    \n",
      "    # Parâmetros de processamento\n",
      "    IMG_SIZE = (224, 224)  # Mesmo tamanho usado no ViT\n",
      "    RANDOM_STATE = 42\n",
      "    TEST_SIZE = 0.2\n",
      "    CV_FOLDS = 5\n",
      "    \n",
      "    # Classes HAM10000 (7 classes como no paper)\n",
      "    HAM10000_CLASSES = {\n",
      "        'akiec': 'Actinic keratoses',\n",
      "        'bcc': 'Basal cell carcinoma', \n",
      "        'bkl': 'Benign keratosis-like lesions',\n",
      "        'df': 'Dermatofibroma',\n",
      "        'mel': 'Melanoma',\n",
      "        'nv': 'Melanocytic nevi',\n",
      "        'vasc': 'Vascular lesions'\n",
      "    }\n",
      "    \n",
      "    # Classes ISIC2019 (8 classes como no paper)\n",
      "    ISIC2019_CLASSES = {\n",
      "        'MEL': 'Melanoma',\n",
      "        'NV': 'Melanocytic nevus',\n",
      "        'BCC': 'Basal cell carcinoma',\n",
      "        'AK': 'Actinic keratosis',\n",
      "        'BKL': 'Benign keratosis',\n",
      "        'DF': 'Dermatofibroma',\n",
      "        'VASC': 'Vascular lesion',\n",
      "        'SCC': 'Squamous cell carcinoma'\n",
      "    }\n",
      "\n",
      "def load_image(image_path, target_size=(224, 224)):\n",
      "    \"\"\"Carrega e pré-processa uma imagem\"\"\"\n",
      "    try:\n",
      "        img = cv2.imread(image_path)\n",
      "        if img is None:\n",
      "            return None\n",
      "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
      "        img = cv2.resize(img, target_size)\n",
      "        return img\n",
      "    except Exception as e:\n",
      "        print(f\"Erro ao carregar imagem {image_path}: {e}\")\n",
      "        return None\n",
      "\n",
      "def extract_color_features(image):\n",
      "    \"\"\"Extrai características de cor da imagem\"\"\"\n",
      "    features = []\n",
      "    \n",
      "    # Estatísticas básicas por canal RGB\n",
      "    for channel in range(3):\n",
      "        channel_data = image[:, :, channel].flatten()\n",
      "        features.extend([\n",
      "            np.mean(channel_data),\n",
      "            np.std(channel_data),\n",
      "            np.median(channel_data),\n",
      "            np.percentile(channel_data, 25),\n",
      "            np.percentile(channel_data, 75)\n",
      "        ])\n",
      "    \n",
      "    # Histograma de cores (bins reduzidos para eficiência)\n",
      "    hist_r = cv2.calcHist([image], [0], None, [16], [0, 256])\n",
      "    hist_g = cv2.calcHist([image], [1], None, [16], [0, 256])\n",
      "    hist_b = cv2.calcHist([image], [2], None, [16], [0, 256])\n",
      "    \n",
      "    features.extend(hist_r.flatten())\n",
      "    features.extend(hist_g.flatten())\n",
      "    features.extend(hist_b.flatten())\n",
      "    \n",
      "    return np.array(features)\n",
      "\n",
      "def extract_texture_features(image):\n",
      "    \"\"\"Extrai características de textura usando filtros simples\"\"\"\n",
      "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
      "    features = []\n",
      "    \n",
      "    # Gradientes\n",
      "    grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
      "    grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
      "    \n",
      "    features.extend([\n",
      "        np.mean(np.abs(grad_x)),\n",
      "        np.std(grad_x),\n",
      "        np.mean(np.abs(grad_y)),\n",
      "        np.std(grad_y)\n",
      "    ])\n",
      "    \n",
      "    # Laplaciano (detecção de bordas)\n",
      "    laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n",
      "    features.extend([\n",
      "        np.mean(np.abs(laplacian)),\n",
      "        np.std(laplacian)\n",
      "    ])\n",
      "    \n",
      "    # Estatísticas básicas da imagem em escala de cinza\n",
      "    features.extend([\n",
      "        np.mean(gray),\n",
      "        np.std(gray),\n",
      "        np.median(gray),\n",
      "        np.percentile(gray, 25),\n",
      "        np.percentile(gray, 75)\n",
      "    ])\n",
      "    \n",
      "    return np.array(features)\n",
      "\n",
      "def extract_shape_features(image):\n",
      "    \"\"\"Extrai características de forma simples\"\"\"\n",
      "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
      "    \n",
      "    # Binarização simples\n",
      "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
      "    \n",
      "    # Contornos\n",
      "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
      "    \n",
      "    features = []\n",
      "    if contours:\n",
      "        # Maior contorno\n",
      "        largest_contour = max(contours, key=cv2.contourArea)\n",
      "        area = cv2.contourArea(largest_contour)\n",
      "        perimeter = cv2.arcLength(largest_contour, True)\n",
      "        \n",
      "        features.extend([\n",
      "            area,\n",
      "            perimeter,\n",
      "            area / (perimeter + 1e-7),  # Compacidade\n",
      "            len(largest_contour)  # Número de pontos do contorno\n",
      "        ])\n",
      "    else:\n",
      "        features.extend([0, 0, 0, 0])\n",
      "    \n",
      "    return np.array(features)\n",
      "```\n",
      "\n",
      "## 2. Carregamento e Processamento dos Datasets\n",
      "\n",
      "```python\n",
      "# data_loader.py\n",
      "import os\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from tqdm import tqdm\n",
      "from utils import Config, load_image, extract_color_features, extract_texture_features, extract_shape_features\n",
      "\n",
      "class DatasetLoader:\n",
      "    def __init__(self):\n",
      "        self.config = Config()\n",
      "    \n",
      "    def load_ham10000(self):\n",
      "        \"\"\"Carrega o dataset HAM10000\"\"\"\n",
      "        print(\"Carregando dataset HAM10000...\")\n",
      "        \n",
      "        # Carrega metadados\n",
      "        metadata_path = os.path.join(self.config.HAM10000_PATH, \"HAM10000_metadata.csv\")\n",
      "        if not os.path.exists(metadata_path):\n",
      "            print(f\"Arquivo de metadados não encontrado: {metadata_path}\")\n",
      "            return None, None\n",
      "        \n",
      "        metadata = pd.read_csv(metadata_path)\n",
      "        \n",
      "        # Diretórios de imagens\n",
      "        img_dirs = [\n",
      "            os.path.join(self.config.HAM10000_PATH, \"HAM10000_images_part_1\"),\n",
      "            os.path.join(self.config.HAM10000_PATH, \"HAM10000_images_part_2\")\n",
      "        ]\n",
      "        \n",
      "        features_list = []\n",
      "        labels_list = []\n",
      "        \n",
      "        for idx, row in tqdm(metadata.iterrows(), total=len(metadata), desc=\"Processando HAM10000\"):\n",
      "            image_id = row['image_id']\n",
      "            label = row['dx']\n",
      "            \n",
      "            # Procura a imagem nos diretórios\n",
      "            image_path = None\n",
      "            for img_dir in img_dirs:\n",
      "                potential_path = os.path.join(img_dir, f\"{image_id}.jpg\")\n",
      "                if os.path.exists(potential_path):\n",
      "                    image_path = potential_path\n",
      "                    break\n",
      "            \n",
      "            if image_path is None:\n",
      "                continue\n",
      "            \n",
      "            # Carrega e processa a imagem\n",
      "            image = load_image(image_path, self.config.IMG_SIZE)\n",
      "            if image is None:\n",
      "                continue\n",
      "            \n",
      "            # Extrai características\n",
      "            color_feat = extract_color_features(image)\n",
      "            texture_feat = extract_texture_features(image)\n",
      "            shape_feat = extract_shape_features(image)\n",
      "            \n",
      "            # Combina todas as características\n",
      "            features = np.concatenate([color_feat, texture_feat, shape_feat])\n",
      "            \n",
      "            features_list.append(features)\n",
      "            labels_list.append(label)\n",
      "        \n",
      "        print(f\"HAM10000: {len(features_list)} imagens processadas\")\n",
      "        return np.array(features_list), np.array(labels_list)\n",
      "    \n",
      "    def load_isic2019(self):\n",
      "        \"\"\"Carrega o dataset ISIC2019\"\"\"\n",
      "        print(\"Carregando dataset ISIC2019...\")\n",
      "        \n",
      "        # Carrega metadados\n",
      "        metadata_path = os.path.join(self.config.ISIC2019_PATH, \"ISIC_2019_Training_GroundTruth.csv\")\n",
      "        if not os.path.exists(metadata_path):\n",
      "            print(f\"Arquivo de metadados não encontrado: {metadata_path}\")\n",
      "            return None, None\n",
      "        \n",
      "        metadata = pd.read_csv(metadata_path)\n",
      "        \n",
      "        # Diretório de imagens\n",
      "        img_dir = os.path.join(self.config.ISIC2019_PATH, \"ISIC_2019_Training_Input\")\n",
      "        \n",
      "        features_list = []\n",
      "        labels_list = []\n",
      "        \n",
      "        for idx, row in tqdm(metadata.iterrows(), total=len(metadata), desc=\"Processando ISIC2019\"):\n",
      "            image_id = row['image']\n",
      "            \n",
      "            # Encontra a classe (coluna com valor 1)\n",
      "            class_cols = ['MEL', 'NV', 'BCC', 'AK', 'BKL', 'DF', 'VASC', 'SCC']\n",
      "            label = None\n",
      "            for col in class_cols:\n",
      "                if row[col] == 1:\n",
      "                    label = col\n",
      "                    break\n",
      "            \n",
      "            if label is None:\n",
      "                continue\n",
      "            \n",
      "            # Carrega imagem\n",
      "            image_path = os.path.join(img_dir, f\"{image_id}.jpg\")\n",
      "            if not os.path.exists(image_path):\n",
      "                continue\n",
      "            \n",
      "            image = load_image(image_path, self.config.IMG_SIZE)\n",
      "            if image is None:\n",
      "                continue\n",
      "            \n",
      "            # Extrai características\n",
      "            color_feat = extract_color_features(image)\n",
      "            texture_feat = extract_texture_features(image)\n",
      "            shape_feat = extract_shape_features(image)\n",
      "            \n",
      "            # Combina todas as características\n",
      "            features = np.concatenate([color_feat, texture_feat, shape_feat])\n",
      "            \n",
      "            features_list.append(features)\n",
      "            labels_list.append(label)\n",
      "        \n",
      "        print(f\"ISIC2019: {len(features_list)} imagens processadas\")\n",
      "        return np.array(features_list), np.array(labels_list)\n",
      "\n",
      "def prepare_data(features, labels):\n",
      "    \"\"\"Prepara os dados para treinamento\"\"\"\n",
      "    # Codifica labels\n",
      "    label_encoder = LabelEncoder()\n",
      "    labels_encoded = label_encoder.fit_transform(labels)\n",
      "    \n",
      "    # Normaliza características\n",
      "    scaler = StandardScaler()\n",
      "    features_scaled = scaler.fit_transform(features)\n",
      "    \n",
      "    return features_scaled, labels_encoded, label_encoder, scaler\n",
      "```\n",
      "\n",
      "## 3. Modelos de Shallow Learning\n",
      "\n",
      "```python\n",
      "# models.py\n",
      "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "import numpy as np\n",
      "\n",
      "class ShallowModels:\n",
      "    def __init__(self, random_state=42):\n",
      "        self.random_state = random_state\n",
      "        self.models = {}\n",
      "        self.best_models = {}\n",
      "        \n",
      "    def initialize_models(self):\n",
      "        \"\"\"Inicializa os modelos com hiperparâmetros para busca\"\"\"\n",
      "        \n",
      "        # Random Forest\n",
      "        self.models['Random Forest'] = {\n",
      "            'model': RandomForestClassifier(random_state=self.random_state),\n",
      "            'params': {\n",
      "                'n_estimators': [100, 200, 300],\n",
      "                'max_depth': [10, 20, None],\n",
      "                'min_samples_split': [2, 5, 10],\n",
      "                'min_samples_leaf': [1, 2, 4]\n",
      "            }\n",
      "        }\n",
      "        \n",
      "        # Gradient Boosting\n",
      "        self.models['Gradient Boosting'] = {\n",
      "            'model': GradientBoostingClassifier(random_state=self.random_state),\n",
      "            'params': {\n",
      "                'n_estimators': [100, 200],\n",
      "                'learning_rate': [0.05, 0.1, 0.2],\n",
      "                'max_depth': [3, 5, 7]\n",
      "            }\n",
      "        }\n",
      "        \n",
      "        # Support Vector Machine\n",
      "        self.models['SVM'] = {\n",
      "            'model': SVC(random_state=self.random_state, probability=True),\n",
      "            'params': {\n",
      "                'C': [0.1, 1, 10, 100],\n",
      "                'kernel': ['rbf', 'poly'],\n",
      "                'gamma': ['scale', 'auto']\n",
      "            }\n",
      "        }\n",
      "        \n",
      "        # Logistic Regression\n",
      "        self.models['Logistic Regression'] = {\n",
      "            'model': LogisticRegression(random_state=self.random_state, max_iter=1000),\n",
      "            'params': {\n",
      "                'C': [0.1, 1, 10, 100],\n",
      "                'penalty': ['l1', 'l2'],\n",
      "                'solver': ['liblinear', 'saga']\n",
      "            }\n",
      "        }\n",
      "        \n",
      "        # K-Nearest Neighbors\n",
      "        self.models['KNN'] = {\n",
      "            'model': KNeighborsClassifier(),\n",
      "            'params': {\n",
      "                'n_neighbors': [3, 5, 7, 9, 11],\n",
      "                'weights': ['uniform', 'distance'],\n",
      "                'metric': ['euclidean', 'manhattan']\n",
      "            }\n",
      "        }\n",
      "        \n",
      "        # Naive Bayes\n",
      "        self.models['Naive Bayes'] = {\n",
      "            'model': GaussianNB(),\n",
      "            'params': {\n",
      "                'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6]\n",
      "            }\n",
      "        }\n",
      "        \n",
      "        # Decision Tree\n",
      "        self.models['Decision Tree'] = {\n",
      "            'model': DecisionTreeClassifier(random_state=self.random_state),\n",
      "            'params': {\n",
      "                'max_depth': [5, 10, 15, 20, None],\n",
      "                'min_samples_split': [2, 5, 10],\n",
      "                'min_samples_leaf': [1, 2, 4]\n",
      "            }\n",
      "        }\n",
      "        \n",
      "        # AdaBoost\n",
      "        self.models['AdaBoost'] = {\n",
      "            'model': AdaBoostClassifier(random_state=self.random_state),\n",
      "            'params': {\n",
      "                'n_estimators': [50, 100, 200],\n",
      "                'learning_rate': [0.5, 1.0, 1.5]\n",
      "            }\n",
      "        }\n",
      "    \n",
      "    def train_with_grid_search(self, X_train, y_train, cv=5, n_jobs=-1):\n",
      "        \"\"\"Treina modelos com busca em grade\"\"\"\n",
      "        print(\"Iniciando treinamento com Grid Search...\")\n",
      "        \n",
      "        for name, model_info in self.models.items():\n",
      "            print(f\"\\nTreinando {name}...\")\n",
      "            \n",
      "            # Grid Search\n",
      "            grid_search = GridSearchCV(\n",
      "                model_info['model'],\n",
      "                model_info['params'],\n",
      "                cv=cv,\n",
      "                scoring='accuracy',\n",
      "                n_jobs=n_jobs,\n",
      "                verbose=1\n",
      "            )\n",
      "            \n",
      "            grid_search.fit(X_train, y_train)\n",
      "            \n",
      "            # Armazena o melhor modelo\n",
      "            self.best_models[name] = {\n",
      "                'model': grid_search.best_estimator_,\n",
      "                'best_params': grid_search.best_params_,\n",
      "                'best_score': grid_search.best_score_\n",
      "            }\n",
      "            \n",
      "            print(f\"{name} - Melhor score CV: {grid_search.best_score_:.4f}\")\n",
      "            print(f\"{name} - Melhores parâmetros: {grid_search.best_params_}\")\n",
      "    \n",
      "    def get_best_models(self):\n",
      "        \"\"\"Retorna os melhores modelos treinados\"\"\"\n",
      "        return self.best_models\n",
      "```\n",
      "\n",
      "## 4. Avaliação e Métricas\n",
      "\n",
      "```python\n",
      "# evaluation.py\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
      "                           f1_score, confusion_matrix, classification_report, \n",
      "                           roc_auc_score, roc_curve, auc)\n",
      "from sklearn.preprocessing import label_binarize\n",
      "from sklearn.model_selection import cross_val_score\n",
      "import joblib\n",
      "\n",
      "class ModelEvaluator:\n",
      "    def __init__(self):\n",
      "        self.results = {}\n",
      "        \n",
      "    def evaluate_model(self, model, model_name, X_test, y_test, class_names=None):\n",
      "        \"\"\"Avalia um modelo individual\"\"\"\n",
      "        \n",
      "        # Predições\n",
      "        y_pred = model.predict(X_test)\n",
      "        y_pred_proba = None\n",
      "        \n",
      "        if hasattr(model, 'predict_proba'):\n",
      "            y_pred_proba = model.predict_proba(X_test)\n",
      "        \n",
      "        # Métricas básicas\n",
      "        accuracy = accuracy_score(y_test, y_pred)\n",
      "        precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
      "        recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
      "        f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
      "        \n",
      "        # AUC (para multiclasse)\n",
      "        auc_score = None\n",
      "        if y_pred_proba is not None:\n",
      "            try:\n",
      "                # Binariza as labels para cálculo do AUC\n",
      "                n_classes = len(np.unique(y_test))\n",
      "                y_test_bin = label_binarize(y_test, classes=range(n_classes))\n",
      "                \n",
      "                if n_classes == 2:\n",
      "                    auc_score = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
      "                else:\n",
      "                    auc_score = roc_auc_score(y_test_bin, y_pred_proba, \n",
      "                                            average='macro', multi_class='ovr')\n",
      "            except:\n",
      "                auc_score = None\n",
      "        \n",
      "        # Armazena resultados\n",
      "        self.results[model_name] = {\n",
      "            'accuracy': accuracy,\n",
      "            'precision': precision,\n",
      "            'recall': recall,\n",
      "            'f1_score': f1,\n",
      "            'auc': auc_score,\n",
      "            'y_pred': y_pred,\n",
      "            'y_pred_proba': y_pred_proba,\n",
      "            'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
      "        }\n",
      "        \n",
      "        return self.results[model_name]\n",
      "    \n",
      "    def evaluate_all_models(self, models_dict, X_test, y_test, class_names=None):\n",
      "        \"\"\"Avalia todos os modelos\"\"\"\n",
      "        print(\"Avaliando modelos...\")\n",
      "        \n",
      "        for name, model_info in models_dict.items():\n",
      "            model = model_info['model']\n",
      "            print(f\"Avaliando {name}...\")\n",
      "            self.evaluate_model(model, name, X_test, y_test, class_names)\n",
      "    \n",
      "    def print_results_summary(self):\n",
      "        \"\"\"Imprime resumo dos resultados\"\"\"\n",
      "        print(\"\\n\" + \"=\"*80)\n",
      "        print(\"RESUMO DOS RESULTADOS\")\n",
      "        print(\"=\"*80)\n",
      "        \n",
      "        # Cria DataFrame com resultados\n",
      "        results_data = []\n",
      "        for model_name, metrics in self.results.items():\n",
      "            results_data.append({\n",
      "                'Modelo': model_name,\n",
      "                'Acurácia': f\"{metrics['accuracy']:.4f}\",\n",
      "                'Precisão': f\"{metrics['precision']:.4f}\",\n",
      "                'Recall': f\"{metrics['recall']:.4f}\",\n",
      "                'F1-Score': f\"{metrics['f1_score']:.4f}\",\n",
      "                'AUC': f\"{metrics['auc']:.4f}\" if metrics['auc'] else \"N/A\"\n",
      "            })\n",
      "        \n",
      "        df_results = pd.DataFrame(results_data)\n",
      "        print(df_results.to_string(index=False))\n",
      "        \n",
      "        # Melhor modelo por métrica\n",
      "        print(f\"\\nMelhor Acurácia: {max(self.results.keys(), key=lambda x: self.results[x]['accuracy'])}\")\n",
      "        print(f\"Melhor F1-Score: {max(self.results.keys(), key=lambda x: self.results[x]['f1_score'])}\")\n",
      "        \n",
      "        if any(self.results[x]['auc'] for x in self.results if self.results[x]['auc']):\n",
      "            best_auc = max([x for x in self.results.keys() if self.results[x]['auc']], \n",
      "                          key=lambda x: self.results[x]['auc'])\n",
      "            print(f\"Melhor AUC: {best_auc}\")\n",
      "    \n",
      "    def plot_results(self, dataset_name=\"Dataset\", save_path=None):\n",
      "        \"\"\"Plota gráficos comparativos\"\"\"\n",
      "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
      "        fig.suptitle(f'Comparação de Modelos - {dataset_name}', fontsize=16, fontweight='bold')\n",
      "        \n",
      "        # Prepara dados para plotagem\n",
      "        models = list(self.results.keys())\n",
      "        metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
      "        metric_names = ['Acurácia', 'Precisão', 'Recall', 'F1-Score']\n",
      "        \n",
      "        for idx, (metric, metric_name) in enumerate(zip(metrics, metric_names)):\n",
      "            ax = axes[idx//2, idx%2]\n",
      "            values = [self.results[model][metric] for model in models]\n",
      "            \n",
      "            bars = ax.bar(models, values, alpha=0.7, color=plt.cm.Set3(np.linspace(0, 1, len(models))))\n",
      "            ax.set_title(metric_name, fontweight='bold')\n",
      "            ax.set_ylabel('Score')\n",
      "            ax.set_ylim(0, 1)\n",
      "            ax.tick_params(axis='x', rotation=45)\n",
      "            \n",
      "            # Adiciona valores nas barras\n",
      "            for bar, value in zip(bars, values):\n",
      "                height = bar.get_height()\n",
      "                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
      "                       f'{value:.3f}', ha='center', va='bottom', fontsize=9)\n",
      "        \n",
      "        plt.tight_layout()\n",
      "        \n",
      "        if save_path:\n",
      "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "        plt.show()\n",
      "    \n",
      "    def plot_confusion_matrices(self, class_names=None, dataset_name=\"Dataset\", save_path=None):\n",
      "        \"\"\"Plota matrizes de confusão para todos os modelos\"\"\"\n",
      "        n_models = len(self.results)\n",
      "        cols = 3\n",
      "        rows = (n_models + cols - 1) // cols\n",
      "        \n",
      "        fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
      "        fig.suptitle(f'Matrizes de Confusão - {dataset_name}', fontsize=16, fontweight='bold')\n",
      "        \n",
      "        if n_models == 1:\n",
      "            axes = [axes]\n",
      "        elif rows == 1:\n",
      "            axes = axes.reshape(1, -1)\n",
      "        \n",
      "        for idx, (model_name, results) in enumerate(self.results.items()):\n",
      "            row, col = idx // cols, idx % cols\n",
      "            ax = axes[row, col] if rows > 1 else axes[col]\n",
      "            \n",
      "            cm = results['confusion_matrix']\n",
      "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
      "                       xticklabels=class_names, yticklabels=class_names)\n",
      "            ax.set_title(f'{model_name}\\nAcc: {results[\"accuracy\"]:.3f}')\n",
      "            ax.set_xlabel('Predito')\n",
      "            ax.set_ylabel('Real')\n",
      "        \n",
      "        # Remove subplots vazios\n",
      "        for idx in range(n_models, rows * cols):\n",
      "            row, col = idx // cols, idx % cols\n",
      "            ax = axes[row, col] if rows > 1 else axes[col]\n",
      "            ax.remove()\n",
      "        \n",
      "        plt.tight_layout()\n",
      "        \n",
      "        if save_path:\n",
      "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "        plt.show()\n",
      "    \n",
      "    def compare_with_paper_results(self, paper_results):\n",
      "        \"\"\"Compara resultados com os do paper\"\"\"\n",
      "        print(\"\\n\" + \"=\"*80)\n",
      "        print(\"COMPARAÇÃO COM RESULTADOS DO PAPER\")\n",
      "        print(\"=\"*80)\n",
      "        \n",
      "        print(f\"Resultados do Paper (ViT Federado):\")\n",
      "        for metric, value in paper_results.items():\n",
      "            print(f\"  {metric}: {value}\")\n",
      "        \n",
      "        print(f\"\\nMelhores Resultados dos Modelos Shallow:\")\n",
      "        best_acc = max(self.results.keys(), key=lambda x: self.results[x]['accuracy'])\n",
      "        best_f1 = max(self.results.keys(), key=lambda x: self.results[x]['f1_score'])\n",
      "        \n",
      "        print(f\"  Melhor Acurácia: {best_acc} ({self.results[best_acc]['accuracy']:.4f})\")\n",
      "        print(f\"  Melhor F1-Score: {best_f1} ({self.results[best_f1]['f1_score']:.4f})\")\n",
      "        \n",
      "        if 'auc' in paper_results:\n",
      "            best_auc_models = [x for x in self.results.keys() if self.results[x]['auc']]\n",
      "            if best_auc_models:\n",
      "                best_auc = max(best_auc_models, key=lambda x: self.results[x]['auc'])\n",
      "                print(f\"  Melhor AUC: {best_auc} ({self.results[best_auc]['auc']:.4f})\")\n",
      "\n",
      "    def save_results(self, filepath):\n",
      "        \"\"\"Salva resultados em arquivo\"\"\"\n",
      "        joblib.dump(self.results, filepath)\n",
      "        print(f\"Resultados salvos em: {filepath}\")\n",
      "    \n",
      "    def load_results(self, filepath):\n",
      "        \"\"\"Carrega resultados de arquivo\"\"\"\n",
      "        self.results = joblib.load(filepath)\n",
      "        print(f\"Resultados carregados de: {filepath}\")\n",
      "```\n",
      "\n",
      "## 5. Script Principal\n",
      "\n",
      "```python\n",
      "# main.py\n",
      "import os\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "import joblib\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from utils import Config\n",
      "from data_loader import DatasetLoader, prepare_data\n",
      "from models import ShallowModels\n",
      "from evaluation import ModelEvaluator\n",
      "\n",
      "def main():\n",
      "    # Configuração\n",
      "    config = Config()\n",
      "    \n",
      "    # Resultados do paper para comparação\n",
      "    paper_results_ham10000 = {\n",
      "        'Global Test Accuracy': '90%',\n",
      "        'Sensitivity': '88.2%',\n",
      "        'Specificity': '91.4%',\n",
      "        'Best AUC': '0.96'\n",
      "    }\n",
      "    \n",
      "    paper_results_isic2019 = {\n",
      "        'Global Test Accuracy': '87.6%',\n",
      "        'Best AUC': '0.96'\n",
      "    }\n",
      "    \n",
      "    # Carregador de dados\n",
      "    loader = DatasetLoader()\n",
      "    \n",
      "    # Processa HAM10000\n",
      "    print(\"=\"*80)\n",
      "    print(\"PROCESSANDO DATASET HAM10000\")\n",
      "    print(\"=\"*80)\n",
      "    \n",
      "    X_ham, y_ham = loader.load_ham10000()\n",
      "    \n",
      "    if X_ham is not None and y_ham is not None:\n",
      "        # Prepara dados\n",
      "        X_ham_scaled, y_ham_encoded, label_encoder_ham, scaler_ham = prepare_data(X_ham, y_ham)\n",
      "        \n",
      "        # Split treino/teste\n",
      "        X_train_ham, X_test_ham, y_train_ham, y_test_ham = train_test_split(\n",
      "            X_ham_scaled, y_ham_encoded, \n",
      "            test_size=config.TEST_SIZE, \n",
      "            random_state=config.RANDOM_STATE,\n",
      "            stratify=y_ham_encoded\n",
      "        )\n",
      "        \n",
      "        print(f\"HAM10000 - Treino: {X_train_ham.shape[0]}, Teste: {X_test_ham.shape[0]}\")\n",
      "        print(f\"Número de características extraídas: {X_ham_scaled.shape[1]}\")\n",
      "        print(f\"Classes: {list(label_encoder_ham.classes_)}\")\n",
      "        \n",
      "        # Treina modelos\n",
      "        models_ham = ShallowModels(random_state=config.RANDOM_STATE)\n",
      "        models_ham.initialize_models()\n",
      "        models_ham.train_with_grid_search(X_train_ham, y_train_ham, cv=config.CV_FOLDS)\n",
      "        \n",
      "        # Avalia modelos\n",
      "        evaluator_ham = ModelEvaluator()\n",
      "        evaluator_ham.evaluate_all_models(\n",
      "            models_ham.get_best_models(), \n",
      "            X_test_ham, \n",
      "            y_test_ham,\n",
      "            class_names=label_encoder_ham.classes_\n",
      "        )\n",
      "        \n",
      "        # Resultados\n",
      "        evaluator_ham.print_results_summary()\n",
      "        evaluator_ham.plot_results(\"HAM10000\", \"results_ham10000_comparison.png\")\n",
      "        evaluator_ham.plot_confusion_matrices(\n",
      "            label_encoder_ham.classes_, \n",
      "            \"HAM10000\", \n",
      "            \"confusion_matrices_ham10000.png\"\n",
      "        )\n",
      "        evaluator_ham.compare_with_paper_results(paper_results_ham10000)\n",
      "        evaluator_ham.save_results(\"results_ham10000.pkl\")\n",
      "        \n",
      "        # Salva modelos e preprocessors\n",
      "        joblib.dump(models_ham.get_best_models(), \"models_ham10000.pkl\")\n",
      "        joblib.dump(label_encoder_ham, \"label_encoder_ham10000.pkl\")\n",
      "        joblib.dump(scaler_ham, \"scaler_ham10000.pkl\")\n",
      "    \n",
      "    # Processa ISIC2019\n",
      "    print(\"\\n\" + \"=\"*80)\n",
      "    print(\"PROCESSANDO DATASET ISIC2019\")\n",
      "    print(\"=\"*80)\n",
      "    \n",
      "    X_isic, y_isic = loader.load_isic2019()\n",
      "    \n",
      "    if X_isic is not None and y_isic is not None:\n",
      "        # Prepara dados\n",
      "        X_isic_scaled, y_isic_encoded, label_encoder_isic, scaler_isic = prepare_data(X_isic, y_isic)\n",
      "        \n",
      "        # Split treino/teste\n",
      "        X_train_isic, X_test_isic, y_train_isic, y_test_isic = train_test_split(\n",
      "            X_isic_scaled, y_isic_encoded,\n",
      "            test_size=config.TEST_SIZE,\n",
      "            random_state=config.RANDOM_STATE,\n",
      "            stratify=y_isic_encoded\n",
      "        )\n",
      "        \n",
      "        print(f\"ISIC2019 - Treino: {X_train_isic.shape[0]}, Teste: {X_test_isic.shape[0]}\")\n",
      "        print(f\"Número de características extraídas: {X_isic_scaled.shape[1]}\")\n",
      "        print(f\"Classes: {list(label_encoder_isic.classes_)}\")\n",
      "        \n",
      "        # Treina modelos\n",
      "        models_isic = ShallowModels(random_state=config.RANDOM_STATE)\n",
      "        models_isic.initialize_models()\n",
      "        models_isic.train_with_grid_search(X_train_isic, y_train_isic, cv=config.CV_FOLDS)\n",
      "        \n",
      "        # Avalia modelos\n",
      "        evaluator_isic = ModelEvaluator()\n",
      "        evaluator_isic.evaluate_all_models(\n",
      "            models_isic.get_best_models(),\n",
      "            X_test_isic,\n",
      "            y_test_isic,\n",
      "            class_names=label_encoder_isic.classes_\n",
      "        )\n",
      "        \n",
      "        # Resultados\n",
      "        evaluator_isic.print_results_summary()\n",
      "        evaluator_isic.plot_results(\"ISIC2019\", \"results_isic2019_comparison.png\")\n",
      "        evaluator_isic.plot_confusion_matrices(\n",
      "            label_encoder_isic.classes_,\n",
      "            \"ISIC2019\",\n",
      "            \"confusion_matrices_isic2019.png\"\n",
      "        )\n",
      "        evaluator_isic.compare_with_paper_results(paper_results_isic2019)\n",
      "        evaluator_isic.save_results(\"results_isic2019.pkl\")\n",
      "        \n",
      "        # Salva modelos e preprocessors\n",
      "        joblib.dump(models_isic.get_best_models(), \"models_isic2019.pkl\")\n",
      "        joblib.dump(label_encoder_isic, \"label_encoder_isic2019.pkl\")\n",
      "        joblib.dump(scaler_isic, \"scaler_isic2019.pkl\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "## 6. Script de Análise Detalhada\n",
      "\n",
      "```python\n",
      "# detailed_analysis.py\n",
      "import joblib\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.metrics import classification_report\n",
      "\n",
      "def analyze_results():\n",
      "    \"\"\"Análise detalhada dos resultados\"\"\"\n",
      "    \n",
      "    # Carrega resultados\n",
      "    try:\n",
      "        results_ham = joblib.load(\"results_ham10000.pkl\")\n",
      "        results_isic = joblib.load(\"results_isic2019.pkl\")\n",
      "        \n",
      "        print(\"=\"*80)\n",
      "        print(\"ANÁLISE DETALHADA DOS RESULTADOS\")\n",
      "        print(\"=\"*80)\n",
      "        \n",
      "        # Análise HAM10000\n",
      "        print(\"\\nHAM10000 - Análise por Classe:\")\n",
      "        print(\"-\" * 40)\n",
      "        \n",
      "        # Carrega label encoder para nomes das classes\n",
      "        label_encoder_ham = joblib.load(\"label_encoder_ham10000.pkl\")\n",
      "        \n",
      "        for model_name, metrics in results_ham.items():\n",
      "            print(f\"\\n{model_name}:\")\n",
      "            # Aqui você pode adicionar análises mais detalhadas por classe\n",
      "            print(f\"  Acurácia Global: {metrics['accuracy']:.4f}\")\n",
      "            print(f\"  F1-Score Macro: {metrics['f1_score']:.4f}\")\n",
      "            if metrics['auc']:\n",
      "                print(f\"  AUC: {metrics['auc']:.4f}\")\n",
      "        \n",
      "        # Análise ISIC2019\n",
      "        print(\"\\n\\nISIC2019 - Análise por Classe:\")\n",
      "        print(\"-\" * 40)\n",
      "        \n",
      "        label_encoder_isic = joblib.load(\"label_encoder_isic2019.pkl\")\n",
      "        \n",
      "        for model_name, metrics in results_isic.items():\n",
      "            print(f\"\\n{model_name}:\")\n",
      "            print(f\"  Acurácia Global: {metrics['accuracy']:.4f}\")\n",
      "            print(f\"  F1-Score Macro: {metrics['f1_score']:.4f}\")\n",
      "            if metrics['auc']:\n",
      "                print(f\"  AUC: {metrics['auc']:.4f}\")\n",
      "        \n",
      "        # Comparação final\n",
      "        print(\"\\n\" + \"=\"*80)\n",
      "        print(\"COMPARAÇÃO FINAL: SHALLOW vs DEEP LEARNING\")\n",
      "        print(\"=\"*80)\n",
      "        \n",
      "        print(\"\\nHAM10000:\")\n",
      "        print(\"  Paper (ViT Federado): 90% acurácia, 88.2% sensibilidade, 91.4% especificidade\")\n",
      "        best_ham = max(results_ham.keys(), key=lambda x: results_ham[x]['accuracy'])\n",
      "        print(f\"  Melhor Shallow ({best_ham}): {results_ham[best_ham]['accuracy']:.1%} acurácia\")\n",
      "        \n",
      "        print(\"\\nISIC2019:\")\n",
      "        print(\"  Paper (ViT Federado): 87.6% acurácia\")\n",
      "        best_isic = max(results_isic.keys(), key=lambda x: results_isic[x]['accuracy'])\n",
      "        print(f\"  Melhor Shallow ({best_isic}): {results_isic[best_isic]['accuracy']:.1%} acurácia\")\n",
      "        \n",
      "    except FileNotFoundError as e:\n",
      "        print(f\"Arquivo não encontrado: {e}\")\n",
      "        print(\"Execute o script main.py primeiro para gerar os resultados.\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    analyze_results()\n",
      "```\n",
      "\n",
      "## 7. README.md\n",
      "\n",
      "```markdown\n",
      "# Replicação: Federated ViT vs Shallow Learning para Classificação de Câncer de Pele\n",
      "\n",
      "Este projeto replica e compara os resultados do artigo \"Federated ViT: A Distributed Deep Learning Framework for Skin Cancer Classification\" usando métodos de **shallow learning** ao invés de deep learning.\n",
      "\n",
      "## Objetivo\n",
      "\n",
      "Comparar o desempenho de métodos tradicionais de machine learning (shallow learning) com os resultados de Vision Transformers federados apresentados no paper original.\n",
      "\n",
      "## Datasets\n",
      "\n",
      "- **HAM10000**: 10.015 imagens, 7 classes de lesões de pele\n",
      "- **ISIC2019**: 25.331 imagens, 8 classes de lesões de pele\n",
      "\n",
      "## Metodologia\n",
      "\n",
      "### Extração de Características\n",
      "- **Características de Cor**: Estatísticas RGB, histogramas\n",
      "- **Características de Textura**: Gradientes, Laplaciano, estatísticas\n",
      "- **Características de Forma**: Área, perímetro, compacidade\n",
      "\n",
      "### Modelos Testados\n",
      "1. Random Forest\n",
      "2. Gradient Boosting\n",
      "3. Support Vector Machine (SVM)\n",
      "4. Logistic Regression\n",
      "5. K-Nearest Neighbors (KNN)\n",
      "6. Naive Bayes\n",
      "7. Decision Tree\n",
      "8. AdaBoost\n",
      "\n",
      "## Execução\n",
      "\n",
      "```bash\n",
      "# Instalar dependências\n",
      "pip install -r requirements.txt\n",
      "\n",
      "# Executar experimento principal\n",
      "python main.py\n",
      "\n",
      "# Análise detalhada dos resultados\n",
      "python detailed_analysis.py\n",
      "```\n",
      "\n",
      "## Estrutura dos Arquivos\n",
      "\n",
      "```\n",
      "├── utils.py              # Utilitários e configurações\n",
      "├── data_loader.py        # Carregamento e processamento dos dados\n",
      "├── models.py             # Modelos de shallow learning\n",
      "├── evaluation.py         # Avaliação e métricas\n",
      "├── main.py               # Script principal\n",
      "├── detailed_analysis.py  # Análise detalhada\n",
      "└── requirements.txt      # Dependências\n",
      "```\n",
      "\n",
      "## Resultados Esperados\n",
      "\n",
      "O projeto gerará:\n",
      "- Comparação de performance entre modelos shallow\n",
      "- Gráficos comparativos de métricas\n",
      "- Matrizes de confusão\n",
      "- Comparação com resultados do paper original\n",
      "- Análise detalhada por classe\n",
      "\n",
      "## Comparação com o Paper Original\n",
      "\n",
      "**Paper (ViT Federado)**:\n",
      "- HAM10000: 90% acurácia global\n",
      "- ISIC2019: 87.6% acurácia global\n",
      "\n",
      "**Este estudo (Shallow Learning)**:\n",
      "- Resultados serão comparados após execução\n",
      "```\n",
      "\n",
      "## Como Executar\n",
      "\n",
      "1. **Prepare os dados**: Certifique-se de que os datasets estão na estrutura correta\n",
      "2. **Instale as dependências**: `pip install -r requirements.txt`\n",
      "3. **Execute o experimento**: `python main.py`\n",
      "4. **Analise os resultados**: `python detailed_analysis.py`\n",
      "\n",
      "Esta replicação permitirá uma comparação direta entre métodos tradicionais de machine learning e o approach de deep learning federado proposto no paper, fornecendo insights sobre quando e por que métodos mais complexos podem ser necessários para classificação de lesões de pele.Com base no artigo analisado, vou criar uma replicação focada em comparar métodos de **shallow learning** com os resultados de **deep learning** (ViT) apresentados no paper. O artigo propõe um framework de Federated Learning com Vision Transformers, mas nossa replicação será centralizada usando métodos tradicionais.\n",
      "\n",
      "## Estrutura da Replicação\n",
      "\n",
      "```python\n",
      "# requirements.txt\n",
      "numpy==1.24.3\n",
      "pandas==2.0.2\n",
      "scikit-learn==1.3.0\n",
      "opencv-python==4.8.0.74\n",
      "matplotlib==3.7.1\n",
      "seaborn==0.12.2\n",
      "pillow==10.0.0\n",
      "joblib==1.3.1\n",
      "tqdm==4.65.0\n",
      "```\n",
      "\n",
      "## 1. Configuração e Utilitários\n",
      "\n",
      "```python\n",
      "# utils.py\n",
      "import os\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import cv2\n",
      "from PIL import Image\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
      "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
      "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_auc_score\n",
      "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "import joblib\n",
      "from tqdm import tqdm\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore')\n",
      "\n",
      "class Config:\n",
      "    \"\"\"Configurações globais do projeto\"\"\"\n",
      "    # Caminhos dos datasets\n",
      "    HAM10000_PATH = \"datasets/HAM10000\"\n",
      "    ISIC2019_PATH = \"datasets/ISIC\"\n",
      "    \n",
      "    # Parâmetros de processamento\n",
      "    IMG_SIZE = (224, 224)  # Mesmo tamanho usado no ViT\n",
      "    RANDOM_STATE = 42\n",
      "    TEST_SIZE = 0.2\n",
      "    CV_FOLDS = 5\n",
      "    \n",
      "    # Classes HAM10000 (7 classes como no paper)\n",
      "    HAM10000_CLASSES = {\n",
      "        'akiec': 'Actinic keratoses',\n",
      "        'bcc': 'Basal cell carcinoma', \n",
      "        'bkl': 'Benign keratosis-like lesions',\n",
      "        'df': 'Dermatofibroma',\n",
      "        'mel': 'Melanoma',\n",
      "        'nv': 'Melanocytic nevi',\n",
      "        'vasc': 'Vascular lesions'\n",
      "    }\n",
      "    \n",
      "    # Classes ISIC2019 (8 classes como no paper)\n",
      "    ISIC2019_CLASSES = {\n",
      "        'MEL': 'Melanoma',\n",
      "        'NV': 'Melanocytic nevus',\n",
      "        'BCC': 'Basal cell carcinoma',\n",
      "        'AK': 'Actinic keratosis',\n",
      "        'BKL': 'Benign keratosis',\n",
      "        'DF': 'Dermatofibroma',\n",
      "        'VASC': 'Vascular lesion',\n",
      "        'SCC': 'Squamous cell carcinoma'\n",
      "    }\n",
      "\n",
      "def load_image(image_path, target_size=(224, 224)):\n",
      "    \"\"\"Carrega e pré-processa uma imagem\"\"\"\n",
      "    try:\n",
      "        img = cv2.imread(image_path)\n",
      "        if img is None:\n",
      "            return None\n",
      "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
      "        img = cv2.resize(img, target_size)\n",
      "        return img\n",
      "    except Exception as e:\n",
      "        print(f\"Erro ao carregar imagem {image_path}: {e}\")\n",
      "        return None\n",
      "\n",
      "def extract_color_features(image):\n",
      "    \"\"\"Extrai características de cor da imagem\"\"\"\n",
      "    features = []\n",
      "    \n",
      "    # Estatísticas básicas por canal RGB\n",
      "    for channel in range(3):\n",
      "        channel_data = image[:, :, channel].flatten()\n",
      "        features.extend([\n",
      "            np.mean(channel_data),\n",
      "            np.std(channel_data),\n",
      "            np.median(channel_data),\n",
      "            np.percentile(channel_data, 25),\n",
      "            np.percentile(channel_data, 75)\n",
      "        ])\n",
      "    \n",
      "    # Histograma de cores (bins reduzidos para eficiência)\n",
      "    hist_r = cv2.calcHist([image], [0], None, [16], [0, 256])\n",
      "    hist_g = cv2.calcHist([image], [1], None, [16], [0, 256])\n",
      "    hist_b = cv2.calcHist([image], [2], None, [16], [0, 256])\n",
      "    \n",
      "    features.extend(hist_r.flatten())\n",
      "    features.extend(hist_g.flatten())\n",
      "    features.extend(hist_b.flatten())\n",
      "    \n",
      "    return np.array(features)\n",
      "\n",
      "def extract_texture_features(image):\n",
      "    \"\"\"Extrai características de textura usando filtros simples\"\"\"\n",
      "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
      "    features = []\n",
      "    \n",
      "    # Gradientes\n",
      "    grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
      "    grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
      "    \n",
      "    features.extend([\n",
      "        np.mean(np.abs(grad_x)),\n",
      "        np.std(grad_x),\n",
      "        np.mean(np.abs(grad_y)),\n",
      "        np.std(grad_y)\n",
      "    ])\n",
      "    \n",
      "    # Laplaciano (detecção de bordas)\n",
      "    laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n",
      "    features.extend([\n",
      "        np.mean(np.abs(laplacian)),\n",
      "        np.std(laplacian)\n",
      "    ])\n",
      "    \n",
      "    # Estatísticas básicas da imagem em escala de cinza\n",
      "    features.extend([\n",
      "        np.mean(gray),\n",
      "        np.std(gray),\n",
      "        np.median(gray),\n",
      "        np.percentile(gray, 25),\n",
      "        np.percentile(gray, 75)\n",
      "    ])\n",
      "    \n",
      "    return np.array(features)\n",
      "\n",
      "def extract_shape_features(image):\n",
      "    \"\"\"Extrai características de forma simples\"\"\"\n",
      "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
      "    \n",
      "    # Binarização simples\n",
      "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
      "    \n",
      "    # Contornos\n",
      "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
      "    \n",
      "    features = []\n",
      "    if contours:\n",
      "        # Maior contorno\n",
      "        largest_contour = max(contours, key=cv2.contourArea)\n",
      "        area = cv2.contourArea(largest_contour)\n",
      "        perimeter = cv2.arcLength(largest_contour, True)\n",
      "        \n",
      "        features.extend([\n",
      "            area,\n",
      "            perimeter,\n",
      "            area / (perimeter + 1e-7),  # Compacidade\n",
      "            len(largest_contour)  # Número de pontos do contorno\n",
      "        ])\n",
      "    else:\n",
      "        features.extend([0, 0, 0, 0])\n",
      "    \n",
      "    return np.array(features)\n",
      "```\n",
      "\n",
      "## 2. Carregamento e Processamento dos Datasets\n",
      "\n",
      "```python\n",
      "# data_loader.py\n",
      "import os\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from tqdm import tqdm\n",
      "from utils import Config, load_image, extract_color_features, extract_texture_features, extract_shape_features\n",
      "\n",
      "class DatasetLoader:\n",
      "    def __init__(self):\n",
      "        self.config = Config()\n",
      "    \n",
      "    def load_ham10000(self):\n",
      "        \"\"\"Carrega o dataset HAM10000\"\"\"\n",
      "        print(\"Carregando dataset HAM10000...\")\n",
      "        \n",
      "        # Carrega metadados\n",
      "        metadata_path = os.path.join(self.config.HAM10000_PATH, \"HAM10000_metadata.csv\")\n",
      "        if not os.path.exists(metadata_path):\n",
      "            print(f\"Arquivo de metadados não encontrado: {metadata_path}\")\n",
      "            return None, None\n",
      "        \n",
      "        metadata = pd.read_csv(metadata_path)\n",
      "        \n",
      "        # Diretórios de imagens\n",
      "        img_dirs = [\n",
      "            os.path.join(self.config.HAM10000_PATH, \"HAM10000_images_part_1\"),\n",
      "            os.path.join(self.config.HAM10000_PATH, \"HAM10000_images_part_2\")\n",
      "        ]\n",
      "        \n",
      "        features_list = []\n",
      "        labels_list = []\n",
      "        \n",
      "        for idx, row in tqdm(metadata.iterrows(), total=len(metadata), desc=\"Processando HAM10000\"):\n",
      "            image_id = row['image_id']\n",
      "            label = row['dx']\n",
      "            \n",
      "            # Procura a imagem nos diretórios\n",
      "            image_path = None\n",
      "            for img_dir in img_dirs:\n",
      "                potential_path = os.path.join(img_dir, f\"{image_id}.jpg\")\n",
      "                if os.path.exists(potential_path):\n",
      "                    image_path = potential_path\n",
      "                    break\n",
      "            \n",
      "            if image_path is None:\n",
      "                continue\n",
      "            \n",
      "            # Carrega e processa a imagem\n",
      "            image = load_image(image_path, self.config.IMG_SIZE)\n",
      "            if image is None:\n",
      "                continue\n",
      "            \n",
      "            # Extrai características\n",
      "            color_feat = extract_color_features(image)\n",
      "            texture_feat = extract_texture_features(image)\n",
      "            shape_feat = extract_shape_features(image)\n",
      "            \n",
      "            # Combina todas as características\n",
      "            features = np.concatenate([color_feat, texture_feat, shape_feat])\n",
      "            \n",
      "            features_list.append(features)\n",
      "            labels_list.append(label)\n",
      "        \n",
      "        print(f\"HAM10000: {len(features_list)} imagens processadas\")\n",
      "        return np.array(features_list), np.array(labels_list)\n",
      "    \n",
      "    def load_isic2019(self):\n",
      "        \"\"\"Carrega o dataset ISIC2019\"\"\"\n",
      "        print(\"Carregando dataset ISIC2019...\")\n",
      "        \n",
      "        # Carrega metadados\n",
      "        metadata_path = os.path.join(self.config.ISIC2019_PATH, \"ISIC_2019_Training_GroundTruth.csv\")\n",
      "        if not os.path.exists(metadata_path):\n",
      "            print(f\"Arquivo de metadados não encontrado: {metadata_path}\")\n",
      "            return None, None\n",
      "        \n",
      "        metadata = pd.read_csv(metadata_path)\n",
      "        \n",
      "        # Diretório de imagens\n",
      "        img_dir = os.path.join(self.config.ISIC2019_PATH, \"ISIC_2019_Training_Input\")\n",
      "        \n",
      "        features_list = []\n",
      "        labels_list = []\n",
      "        \n",
      "        for idx, row in tqdm(metadata.iterrows(), total=len(metadata), desc=\"Processando ISIC2019\"):\n",
      "            image_id = row['image']\n",
      "            \n",
      "            # Encontra a classe (coluna com valor 1)\n",
      "            class_cols = ['MEL', 'NV', 'BCC', 'AK', 'BKL', 'DF', 'VASC', 'SCC']\n",
      "            label = None\n",
      "            for col in class_cols:\n",
      "                if row[col] == 1:\n",
      "                    label = col\n",
      "                    break\n",
      "            \n",
      "            if label is None:\n",
      "                continue\n",
      "            \n",
      "            # Carrega imagem\n",
      "            image_path = os.path.join(img_dir, f\"{image_id}.jpg\")\n",
      "            if not os.path.exists(image_path):\n",
      "                continue\n",
      "            \n",
      "            image = load_image(image_path, self.config.IMG_SIZE)\n",
      "            if image is None:\n",
      "                continue\n",
      "            \n",
      "            # Extrai características\n",
      "            color_feat = extract_color_features(image)\n",
      "            texture_feat = extract_texture_features(image)\n",
      "            shape_feat = extract_shape_features(image)\n",
      "            \n",
      "            # Combina todas as características\n",
      "            features = np.concatenate([color_feat, texture_feat, shape_feat])\n",
      "            \n",
      "            features_list.append(features)\n",
      "            labels_list.append(label)\n",
      "        \n",
      "        print(f\"ISIC2019: {len(features_list)} imagens processadas\")\n",
      "        return np.array(features_list), np.array(labels_list)\n",
      "\n",
      "def prepare_data(features, labels):\n",
      "    \"\"\"Prepara os dados para treinamento\"\"\"\n",
      "    # Codifica labels\n",
      "    label_encoder = LabelEncoder()\n",
      "    labels_encoded = label_encoder.fit_transform(labels)\n",
      "    \n",
      "    # Normaliza características\n",
      "    scaler = StandardScaler()\n",
      "    features_scaled = scaler.fit_transform(features)\n",
      "    \n",
      "    return features_scaled, labels_encoded, label_encoder, scaler\n",
      "```\n",
      "\n",
      "## 3. Modelos de Shallow Learning\n",
      "\n",
      "```python\n",
      "# models.py\n",
      "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
      "from sklearn.svm import SVC\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from sklearn.naive_bayes import GaussianNB\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "import numpy as np\n",
      "\n",
      "class ShallowModels:\n",
      "    def __init__(self, random_state=42):\n",
      "        self.random_state = random_state\n",
      "        self.models = {}\n",
      "        self.best_models = {}\n",
      "        \n",
      "    def initialize_models(self):\n",
      "        \"\"\"Inicializa os modelos com hiperparâmetros para busca\"\"\"\n",
      "        \n",
      "        # Random Forest\n",
      "        self.models['Random Forest'] = {\n",
      "            'model': RandomForestClassifier(random_state=self.random_state),\n",
      "            'params': {\n",
      "                'n_estimators': [100, 200, 300],\n",
      "                'max_depth': [10, 20, None],\n",
      "                'min_samples_split': [2, 5, 10],\n",
      "                'min_samples_leaf': [1, 2, 4]\n",
      "            }\n",
      "        }\n",
      "        \n",
      "        # Gradient Boosting\n",
      "        self.models['Gradient Boosting'] = {\n",
      "            'model': GradientBoostingClassifier(random_state=self.random_state),\n",
      "            'params': {\n",
      "                'n_estimators': [100, 200],\n",
      "                'learning_rate': [0.05, 0.1, 0.2],\n",
      "                'max_depth': [3, 5, 7]\n",
      "            }\n",
      "        }\n",
      "        \n",
      "        # Support Vector Machine\n",
      "        self.models['SVM'] = {\n",
      "            'model': SVC(random_state=self.random_state, probability=True),\n",
      "            'params': {\n",
      "                'C': [0.1, 1, 10, 100],\n",
      "                'kernel': ['rbf', 'poly'],\n",
      "                'gamma': ['scale', 'auto']\n",
      "            }\n",
      "        }\n",
      "        \n",
      "        # Logistic Regression\n",
      "        self.models['Logistic Regression'] = {\n",
      "            'model': LogisticRegression(random_state=self.random_state, max_iter=1000),\n",
      "            'params': {\n",
      "                'C': [0.1, 1, 10, 100],\n",
      "                'penalty': ['l1', 'l2'],\n",
      "                'solver': ['liblinear', 'saga']\n",
      "            }\n",
      "        }\n",
      "        \n",
      "        # K-Nearest Neighbors\n",
      "        self.models['KNN'] = {\n",
      "            'model': KNeighborsClassifier(),\n",
      "            'params': {\n",
      "                'n_neighbors': [3, 5, 7, 9, 11],\n",
      "                'weights': ['uniform', 'distance'],\n",
      "                'metric': ['euclidean', 'manhattan']\n",
      "            }\n",
      "        }\n",
      "        \n",
      "        # Naive Bayes\n",
      "        self.models['Naive Bayes'] = {\n",
      "            'model': GaussianNB(),\n",
      "            'params': {\n",
      "                'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6]\n",
      "            }\n",
      "        }\n",
      "        \n",
      "        # Decision Tree\n",
      "        self.models['Decision Tree'] = {\n",
      "            'model': DecisionTreeClassifier(random_state=self.random_state),\n",
      "            'params': {\n",
      "                'max_depth': [5, 10, 15, 20, None],\n",
      "                'min_samples_split': [2, 5, 10],\n",
      "                'min_samples_leaf': [1, 2, 4]\n",
      "            }\n",
      "        }\n",
      "        \n",
      "        # AdaBoost\n",
      "        self.models['AdaBoost'] = {\n",
      "            'model': AdaBoostClassifier(random_state=self.random_state),\n",
      "            'params': {\n",
      "                'n_estimators': [50, 100, 200],\n",
      "                'learning_rate': [0.5, 1.0, 1.5]\n",
      "            }\n",
      "        }\n",
      "    \n",
      "    def train_with_grid_search(self, X_train, y_train, cv=5, n_jobs=-1):\n",
      "        \"\"\"Treina modelos com busca em grade\"\"\"\n",
      "        print(\"Iniciando treinamento com Grid Search...\")\n",
      "        \n",
      "        for name, model_info in self.models.items():\n",
      "            print(f\"\\nTreinando {name}...\")\n",
      "            \n",
      "            # Grid Search\n",
      "            grid_search = GridSearchCV(\n",
      "                model_info['model'],\n",
      "                model_info['params'],\n",
      "                cv=cv,\n",
      "                scoring='accuracy',\n",
      "                n_jobs=n_jobs,\n",
      "                verbose=1\n",
      "            )\n",
      "            \n",
      "            grid_search.fit(X_train, y_train)\n",
      "            \n",
      "            # Armazena o melhor modelo\n",
      "            self.best_models[name] = {\n",
      "                'model': grid_search.best_estimator_,\n",
      "                'best_params': grid_search.best_params_,\n",
      "                'best_score': grid_search.best_score_\n",
      "            }\n",
      "            \n",
      "            print(f\"{name} - Melhor score CV: {grid_search.best_score_:.4f}\")\n",
      "            print(f\"{name} - Melhores parâmetros: {grid_search.best_params_}\")\n",
      "    \n",
      "    def get_best_models(self):\n",
      "        \"\"\"Retorna os melhores modelos treinados\"\"\"\n",
      "        return self.best_models\n",
      "```\n",
      "\n",
      "## 4. Avaliação e Métricas\n",
      "\n",
      "```python\n",
      "# evaluation.py\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
      "                           f1_score, confusion_matrix, classification_report, \n",
      "                           roc_auc_score, roc_curve, auc)\n",
      "from sklearn.preprocessing import label_binarize\n",
      "from sklearn.model_selection import cross_val_score\n",
      "import joblib\n",
      "\n",
      "class ModelEvaluator:\n",
      "    def __init__(self):\n",
      "        self.results = {}\n",
      "        \n",
      "    def evaluate_model(self, model, model_name, X_test, y_test, class_names=None):\n",
      "        \"\"\"Avalia um modelo individual\"\"\"\n",
      "        \n",
      "        # Predições\n",
      "        y_pred = model.predict(X_test)\n",
      "        y_pred_proba = None\n",
      "        \n",
      "        if hasattr(model, 'predict_proba'):\n",
      "            y_pred_proba = model.predict_proba(X_test)\n",
      "        \n",
      "        # Métricas básicas\n",
      "        accuracy = accuracy_score(y_test, y_pred)\n",
      "        precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
      "        recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
      "        f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
      "        \n",
      "        # AUC (para multiclasse)\n",
      "        auc_score = None\n",
      "        if y_pred_proba is not None:\n",
      "            try:\n",
      "                # Binariza as labels para cálculo do AUC\n",
      "                n_classes = len(np.unique(y_test))\n",
      "                y_test_bin = label_binarize(y_test, classes=range(n_classes))\n",
      "                \n",
      "                if n_classes == 2:\n",
      "                    auc_score = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
      "                else:\n",
      "                    auc_score = roc_auc_score(y_test_bin, y_pred_proba, \n",
      "                                            average='macro', multi_class='ovr')\n",
      "            except:\n",
      "                auc_score = None\n",
      "        \n",
      "        # Armazena resultados\n",
      "        self.results[model_name] = {\n",
      "            'accuracy': accuracy,\n",
      "            'precision': precision,\n",
      "            'recall': recall,\n",
      "            'f1_score': f1,\n",
      "            'auc': auc_score,\n",
      "            'y_pred': y_pred,\n",
      "            'y_pred_proba': y_pred_proba,\n",
      "            'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
      "        }\n",
      "        \n",
      "        return self.results[model_name]\n",
      "    \n",
      "    def evaluate_all_models(self, models_dict, X_test, y_test, class_names=None):\n",
      "        \"\"\"Avalia todos os modelos\"\"\"\n",
      "        print(\"Avaliando modelos...\")\n",
      "        \n",
      "        for name, model_info in models_dict.items():\n",
      "            model = model_info['model']\n",
      "            print(f\"Avaliando {name}...\")\n",
      "            self.evaluate_model(model, name, X_test, y_test, class_names)\n",
      "    \n",
      "    def print_results_summary(self):\n",
      "        \"\"\"Imprime resumo dos resultados\"\"\"\n",
      "        print(\"\\n\" + \"=\"*80)\n",
      "        print(\"RESUMO DOS RESULTADOS\")\n",
      "        print(\"=\"*80)\n",
      "        \n",
      "        # Cria DataFrame com resultados\n",
      "        results_data = []\n",
      "        for model_name, metrics in self.results.items():\n",
      "            results_data.append({\n",
      "                'Modelo': model_name,\n",
      "                'Acurácia': f\"{metrics['accuracy']:.4f}\",\n",
      "                'Precisão': f\"{metrics['precision']:.4f}\",\n",
      "                'Recall': f\"{metrics['recall']:.4f}\",\n",
      "                'F1-Score': f\"{metrics['f1_score']:.4f}\",\n",
      "                'AUC': f\"{metrics['auc']:.4f}\" if metrics['auc'] else \"N/A\"\n",
      "            })\n",
      "        \n",
      "        df_results = pd.DataFrame(results_data)\n",
      "        print(df_results.to_string(index=False))\n",
      "        \n",
      "        # Melhor modelo por métrica\n",
      "        print(f\"\\nMelhor Acurácia: {max(self.results.keys(), key=lambda x: self.results[x]['accuracy'])}\")\n",
      "        print(f\"Melhor F1-Score: {max(self.results.keys(), key=lambda x: self.results[x]['f1_score'])}\")\n",
      "        \n",
      "        if any(self.results[x]['auc'] for x in self.results if self.results[x]['auc']):\n",
      "            best_auc = max([x for x in self.results.keys() if self.results[x]['auc']], \n",
      "                          key=lambda x: self.results[x]['auc'])\n",
      "            print(f\"Melhor AUC: {best_auc}\")\n",
      "    \n",
      "    def plot_results(self, dataset_name=\"Dataset\", save_path=None):\n",
      "        \"\"\"Plota gráficos comparativos\"\"\"\n",
      "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
      "        fig.suptitle(f'Comparação de Modelos - {dataset_name}', fontsize=16, fontweight='bold')\n",
      "        \n",
      "        # Prepara dados para plotagem\n",
      "        models = list(self.results.keys())\n",
      "        metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
      "        metric_names = ['Acurácia', 'Precisão', 'Recall', 'F1-Score']\n",
      "        \n",
      "        for idx, (metric, metric_name) in enumerate(zip(metrics, metric_names)):\n",
      "            ax = axes[idx//2, idx%2]\n",
      "            values = [self.results[model][metric] for model in models]\n",
      "            \n",
      "            bars = ax.bar(models, values, alpha=0.7, color=plt.cm.Set3(np.linspace(0, 1, len(models))))\n",
      "            ax.set_title(metric_name, fontweight='bold')\n",
      "            ax.set_ylabel('Score')\n",
      "            ax.set_ylim(0, 1)\n",
      "            ax.tick_params(axis='x', rotation=45)\n",
      "            \n",
      "            # Adiciona valores nas barras\n",
      "            for bar, value in zip(bars, values):\n",
      "                height = bar.get_height()\n",
      "                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
      "                       f'{value:.3f}', ha='center', va='bottom', fontsize=9)\n",
      "        \n",
      "        plt.tight_layout()\n",
      "        \n",
      "        if save_path:\n",
      "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "        plt.show()\n",
      "    \n",
      "    def plot_confusion_matrices(self, class_names=None, dataset_name=\"Dataset\", save_path=None):\n",
      "        \"\"\"Plota matrizes de confusão para todos os modelos\"\"\"\n",
      "        n_models = len(self.results)\n",
      "        cols = 3\n",
      "        rows = (n_models + cols - 1) // cols\n",
      "        \n",
      "        fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
      "        fig.suptitle(f'Matrizes de Confusão - {dataset_name}', fontsize=16, fontweight='bold')\n",
      "        \n",
      "        if n_models == 1:\n",
      "            axes = [axes]\n",
      "        elif rows == 1:\n",
      "            axes = axes.reshape(1, -1)\n",
      "        \n",
      "        for idx, (model_name, results) in enumerate(self.results.items()):\n",
      "            row, col = idx // cols, idx % cols\n",
      "            ax = axes[row, col] if rows > 1 else axes[col]\n",
      "            \n",
      "            cm = results['confusion_matrix']\n",
      "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
      "                       xticklabels=class_names, yticklabels=class_names)\n",
      "            ax.set_title(f'{model_name}\\nAcc: {results[\"accuracy\"]:.3f}')\n",
      "            ax.set_xlabel('Predito')\n",
      "            ax.set_ylabel('Real')\n",
      "        \n",
      "        # Remove subplots vazios\n",
      "        for idx in range(n_models, rows * cols):\n",
      "            row, col = idx // cols, idx % cols\n",
      "            ax = axes[row, col] if rows > 1 else axes[col]\n",
      "            ax.remove()\n",
      "        \n",
      "        plt.tight_layout()\n",
      "        \n",
      "        if save_path:\n",
      "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
      "        plt.show()\n",
      "    \n",
      "    def compare_with_paper_results(self, paper_results):\n",
      "        \"\"\"Compara resultados com os do paper\"\"\"\n",
      "        print(\"\\n\" + \"=\"*80)\n",
      "        print(\"COMPARAÇÃO COM RESULTADOS DO PAPER\")\n",
      "        print(\"=\"*80)\n",
      "        \n",
      "        print(f\"Resultados do Paper (ViT Federado):\")\n",
      "        for metric, value in paper_results.items():\n",
      "            print(f\"  {metric}: {value}\")\n",
      "        \n",
      "        print(f\"\\nMelhores Resultados dos Modelos Shallow:\")\n",
      "        best_acc = max(self.results.keys(), key=lambda x: self.results[x]['accuracy'])\n",
      "        best_f1 = max(self.results.keys(), key=lambda x: self.results[x]['f1_score'])\n",
      "        \n",
      "        print(f\"  Melhor Acurácia: {best_acc} ({self.results[best_acc]['accuracy']:.4f})\")\n",
      "        print(f\"  Melhor F1-Score: {best_f1} ({self.results[best_f1]['f1_score']:.4f})\")\n",
      "        \n",
      "        if 'auc' in paper_results:\n",
      "            best_auc_models = [x for x in self.results.keys() if self.results[x]['auc']]\n",
      "            if best_auc_models:\n",
      "                best_auc = max(best_auc_models, key=lambda x: self.results[x]['auc'])\n",
      "                print(f\"  Melhor AUC: {best_auc} ({self.results[best_auc]['auc']:.4f})\")\n",
      "\n",
      "    def save_results(self, filepath):\n",
      "        \"\"\"Salva resultados em arquivo\"\"\"\n",
      "        joblib.dump(self.results, filepath)\n",
      "        print(f\"Resultados salvos em: {filepath}\")\n",
      "    \n",
      "    def load_results(self, filepath):\n",
      "        \"\"\"Carrega resultados de arquivo\"\"\"\n",
      "        self.results = joblib.load(filepath)\n",
      "        print(f\"Resultados carregados de: {filepath}\")\n",
      "```\n",
      "\n",
      "## 5. Script Principal\n",
      "\n",
      "```python\n",
      "# main.py\n",
      "import os\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "import joblib\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from utils import Config\n",
      "from data_loader import DatasetLoader, prepare_data\n",
      "from models import ShallowModels\n",
      "from evaluation import ModelEvaluator\n",
      "\n",
      "def main():\n",
      "    # Configuração\n",
      "    config = Config()\n",
      "    \n",
      "    # Resultados do paper para comparação\n",
      "    paper_results_ham10000 = {\n",
      "        'Global Test Accuracy': '90%',\n",
      "        'Sensitivity': '88.2%',\n",
      "        'Specificity': '91.4%',\n",
      "        'Best AUC': '0.96'\n",
      "    }\n",
      "    \n",
      "    paper_results_isic2019 = {\n",
      "        'Global Test Accuracy': '87.6%',\n",
      "        'Best AUC': '0.96'\n",
      "    }\n",
      "    \n",
      "    # Carregador de dados\n",
      "    loader = DatasetLoader()\n",
      "    \n",
      "    # Processa HAM10000\n",
      "    print(\"=\"*80)\n",
      "    print(\"PROCESSANDO DATASET HAM10000\")\n",
      "    print(\"=\"*80)\n",
      "    \n",
      "    X_ham, y_ham = loader.load_ham10000()\n",
      "    \n",
      "    if X_ham is not None and y_ham is not None:\n",
      "        # Prepara dados\n",
      "        X_ham_scaled, y_ham_encoded, label_encoder_ham, scaler_ham = prepare_data(X_ham, y_ham)\n",
      "        \n",
      "        # Split treino/teste\n",
      "        X_train_ham, X_test_ham, y_train_ham, y_test_ham = train_test_split(\n",
      "            X_ham_scaled, y_ham_encoded, \n",
      "            test_size=config.TEST_SIZE, \n",
      "            random_state=config.RANDOM_STATE,\n",
      "            stratify=y_ham_encoded\n",
      "        )\n",
      "        \n",
      "        print(f\"HAM10000 - Treino: {X_train_ham.shape[0]}, Teste: {X_test_ham.shape[0]}\")\n",
      "        print(f\"Número de características extraídas: {X_ham_scaled.shape[1]}\")\n",
      "        print(f\"Classes: {list(label_encoder_ham.classes_)}\")\n",
      "        \n",
      "        # Treina modelos\n",
      "        models_ham = ShallowModels(random_state=config.RANDOM_STATE)\n",
      "        models_ham.initialize_models()\n",
      "        models_ham.train_with_grid_search(X_train_ham, y_train_ham, cv=config.CV_FOLDS)\n",
      "        \n",
      "        # Avalia modelos\n",
      "        evaluator_ham = ModelEvaluator()\n",
      "        evaluator_ham.evaluate_all_models(\n",
      "            models_ham.get_best_models(), \n",
      "            X_test_ham, \n",
      "            y_test_ham,\n",
      "            class_names=label_encoder_ham.classes_\n",
      "        )\n",
      "        \n",
      "        # Resultados\n",
      "        evaluator_ham.print_results_summary()\n",
      "        evaluator_ham.plot_results(\"HAM10000\", \"results_ham10000_comparison.png\")\n",
      "        evaluator_ham.plot_confusion_matrices(\n",
      "            label_encoder_ham.classes_, \n",
      "            \"HAM10000\", \n",
      "            \"confusion_matrices_ham10000.png\"\n",
      "        )\n",
      "        evaluator_ham.compare_with_paper_results(paper_results_ham10000)\n",
      "        evaluator_ham.save_results(\"results_ham10000.pkl\")\n",
      "        \n",
      "        # Salva modelos e preprocessors\n",
      "        joblib.dump(models_ham.get_best_models(), \"models_ham10000.pkl\")\n",
      "        joblib.dump(label_encoder_ham, \"label_encoder_ham10000.pkl\")\n",
      "        joblib.dump(scaler_ham, \"scaler_ham10000.pkl\")\n",
      "    \n",
      "    # Processa ISIC2019\n",
      "    print(\"\\n\" + \"=\"*80)\n",
      "    print(\"PROCESSANDO DATASET ISIC2019\")\n",
      "    print(\"=\"*80)\n",
      "    \n",
      "    X_isic, y_isic = loader.load_isic2019()\n",
      "    \n",
      "    if X_isic is not None and y_isic is not None:\n",
      "        # Prepara dados\n",
      "        X_isic_scaled, y_isic_encoded, label_encoder_isic, scaler_isic = prepare_data(X_isic, y_isic)\n",
      "        \n",
      "        # Split treino/teste\n",
      "        X_train_isic, X_test_isic, y_train_isic, y_test_isic = train_test_split(\n",
      "            X_isic_scaled, y_isic_encoded,\n",
      "            test_size=config.TEST_SIZE,\n",
      "            random_state=config.RANDOM_STATE,\n",
      "            stratify=y_isic_encoded\n",
      "        )\n",
      "        \n",
      "        print(f\"ISIC2019 - Treino: {X_train_isic.shape[0]}, Teste: {X_test_isic.shape[0]}\")\n",
      "        print(f\"Número de características extraídas: {X_isic_scaled.shape[1]}\")\n",
      "        print(f\"Classes: {list(label_encoder_isic.classes_)}\")\n",
      "        \n",
      "        # Treina modelos\n",
      "        models_isic = ShallowModels(random_state=config.RANDOM_STATE)\n",
      "        models_isic.initialize_models()\n",
      "        models_isic.train_with_grid_search(X_train_isic, y_train_isic, cv=config.CV_FOLDS)\n",
      "        \n",
      "        # Avalia modelos\n",
      "        evaluator_isic = ModelEvaluator()\n",
      "        evaluator_isic.evaluate_all_models(\n",
      "            models_isic.get_best_models(),\n",
      "            X_test_isic,\n",
      "            y_test_isic,\n",
      "            class_names=label_encoder_isic.classes_\n",
      "        )\n",
      "        \n",
      "        # Resultados\n",
      "        evaluator_isic.print_results_summary()\n",
      "        evaluator_isic.plot_results(\"ISIC2019\", \"results_isic2019_comparison.png\")\n",
      "        evaluator_isic.plot_confusion_matrices(\n",
      "            label_encoder_isic.classes_,\n",
      "            \"ISIC2019\",\n",
      "            \"confusion_matrices_isic2019.png\"\n",
      "        )\n",
      "        evaluator_isic.compare_with_paper_results(paper_results_isic2019)\n",
      "        evaluator_isic.save_results(\"results_isic2019.pkl\")\n",
      "        \n",
      "        # Salva modelos e preprocessors\n",
      "        joblib.dump(models_isic.get_best_models(), \"models_isic2019.pkl\")\n",
      "        joblib.dump(label_encoder_isic, \"label_encoder_isic2019.pkl\")\n",
      "        joblib.dump(scaler_isic, \"scaler_isic2019.pkl\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "## 6. Script de Análise Detalhada\n",
      "\n",
      "```python\n",
      "# detailed_analysis.py\n",
      "import joblib\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.metrics import classification_report\n",
      "\n",
      "def analyze_results():\n",
      "    \"\"\"Análise detalhada dos resultados\"\"\"\n",
      "    \n",
      "    # Carrega resultados\n",
      "    try:\n",
      "        results_ham = joblib.load(\"results_ham10000.pkl\")\n",
      "        results_isic = joblib.load(\"results_isic2019.pkl\")\n",
      "        \n",
      "        print(\"=\"*80)\n",
      "        print(\"ANÁLISE DETALHADA DOS RESULTADOS\")\n",
      "        print(\"=\"*80)\n",
      "        \n",
      "        # Análise HAM10000\n",
      "        print(\"\\nHAM10000 - Análise por Classe:\")\n",
      "        print(\"-\" * 40)\n",
      "        \n",
      "        # Carrega label encoder para nomes das classes\n",
      "        label_encoder_ham = joblib.load(\"label_encoder_ham10000.pkl\")\n",
      "        \n",
      "        for model_name, metrics in results_ham.items():\n",
      "            print(f\"\\n{model_name}:\")\n",
      "            # Aqui você pode adicionar análises mais detalhadas por classe\n",
      "            print(f\"  Acurácia Global: {metrics['accuracy']:.4f}\")\n",
      "            print(f\"  F1-Score Macro: {metrics['f1_score']:.4f}\")\n",
      "            if metrics['auc']:\n",
      "                print(f\"  AUC: {metrics['auc']:.4f}\")\n",
      "        \n",
      "        # Análise ISIC2019\n",
      "        print(\"\\n\\nISIC2019 - Análise por Classe:\")\n",
      "        print(\"-\" * 40)\n",
      "        \n",
      "        label_encoder_isic = joblib.load(\"label_encoder_isic2019.pkl\")\n",
      "        \n",
      "        for model_name, metrics in results_isic.items():\n",
      "            print(f\"\\n{model_name}:\")\n",
      "            print(f\"  Acurácia Global: {metrics['accuracy']:.4f}\")\n",
      "            print(f\"  F1-Score Macro: {metrics['f1_score']:.4f}\")\n",
      "            if metrics['auc']:\n",
      "                print(f\"  AUC: {metrics['auc']:.4f}\")\n",
      "        \n",
      "        # Comparação final\n",
      "        print(\"\\n\" + \"=\"*80)\n",
      "        print(\"COMPARAÇÃO FINAL: SHALLOW vs DEEP LEARNING\")\n",
      "        print(\"=\"*80)\n",
      "        \n",
      "        print(\"\\nHAM10000:\")\n",
      "        print(\"  Paper (ViT Federado): 90% acurácia, 88.2% sensibilidade, 91.4% especificidade\")\n",
      "        best_ham = max(results_ham.keys(), key=lambda x: results_ham[x]['accuracy'])\n",
      "        print(f\"  Melhor Shallow ({best_ham}): {results_ham[best_ham]['accuracy']:.1%} acurácia\")\n",
      "        \n",
      "        print(\"\\nISIC2019:\")\n",
      "        print(\"  Paper (ViT Federado): 87.6% acurácia\")\n",
      "        best_isic = max(results_isic.keys(), key=lambda x: results_isic[x]['accuracy'])\n",
      "        print(f\"  Melhor Shallow ({best_isic}): {results_isic[best_isic]['accuracy']:.1%} acurácia\")\n",
      "        \n",
      "    except FileNotFoundError as e:\n",
      "        print(f\"Arquivo não encontrado: {e}\")\n",
      "        print(\"Execute o script main.py primeiro para gerar os resultados.\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    analyze_results()\n",
      "```\n",
      "\n",
      "## 7. README.md\n",
      "\n",
      "```markdown\n",
      "# Replicação: Federated ViT vs Shallow Learning para Classificação de Câncer de Pele\n",
      "\n",
      "Este projeto replica e compara os resultados do artigo \"Federated ViT: A Distributed Deep Learning Framework for Skin Cancer Classification\" usando métodos de **shallow learning** ao invés de deep learning.\n",
      "\n",
      "## Objetivo\n",
      "\n",
      "Comparar o desempenho de métodos tradicionais de machine learning (shallow learning) com os resultados de Vision Transformers federados apresentados no paper original.\n",
      "\n",
      "## Datasets\n",
      "\n",
      "- **HAM10000**: 10.015 imagens, 7 classes de lesões de pele\n",
      "- **ISIC2019**: 25.331 imagens, 8 classes de lesões de pele\n",
      "\n",
      "## Metodologia\n",
      "\n",
      "### Extração de Características\n",
      "- **Características de Cor**: Estatísticas RGB, histogramas\n",
      "- **Características de Textura**: Gradientes, Laplaciano, estatísticas\n",
      "- **Características de Forma**: Área, perímetro, compacidade\n",
      "\n",
      "### Modelos Testados\n",
      "1. Random Forest\n",
      "2. Gradient Boosting\n",
      "3. Support Vector Machine (SVM)\n",
      "4. Logistic Regression\n",
      "5. K-Nearest Neighbors (KNN)\n",
      "6. Naive Bayes\n",
      "7. Decision Tree\n",
      "8. AdaBoost\n",
      "\n",
      "## Execução\n",
      "\n",
      "```bash\n",
      "# Instalar dependências\n",
      "pip install -r requirements.txt\n",
      "\n",
      "# Executar experimento principal\n",
      "python main.py\n",
      "\n",
      "# Análise detalhada dos resultados\n",
      "python detailed_analysis.py\n",
      "```\n",
      "\n",
      "## Estrutura dos Arquivos\n",
      "\n",
      "```\n",
      "├── utils.py              # Utilitários e configurações\n",
      "├── data_loader.py        # Carregamento e processamento dos dados\n",
      "├── models.py             # Modelos de shallow learning\n",
      "├── evaluation.py         # Avaliação e métricas\n",
      "├── main.py               # Script principal\n",
      "├── detailed_analysis.py  # Análise detalhada\n",
      "└── requirements.txt      # Dependências\n",
      "```\n",
      "\n",
      "## Resultados Esperados\n",
      "\n",
      "O projeto gerará:\n",
      "- Comparação de performance entre modelos shallow\n",
      "- Gráficos comparativos de métricas\n",
      "- Matrizes de confusão\n",
      "- Comparação com resultados do paper original\n",
      "- Análise detalhada por classe\n",
      "\n",
      "## Comparação com o Paper Original\n",
      "\n",
      "**Paper (ViT Federado)**:\n",
      "- HAM10000: 90% acurácia global\n",
      "- ISIC2019: 87.6% acurácia global\n",
      "\n",
      "**Este estudo (Shallow Learning)**:\n",
      "- Resultados serão comparados após execução\n",
      "```\n",
      "\n",
      "## Como Executar\n",
      "\n",
      "1. **Prepare os dados**: Certifique-se de que os datasets estão na estrutura correta\n",
      "2. **Instale as dependências**: `pip install -r requirements.txt`\n",
      "3. **Execute o experimento**: `python main.py`\n",
      "4. **Analise os resultados**: `python detailed_analysis.py`\n",
      "\n",
      "Esta replicação permitirá uma comparação direta entre métodos tradicionais de machine learning e o approach de deep learning federado proposto no paper, fornecendo insights sobre quando e por que métodos mais complexos podem ser necessários para classificação de lesões de pele.\n"
     ]
    }
   ],
   "source": [
    "response = run_claude_stream(zero_rules, system_prompt) \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56d3c480",
   "metadata": {},
   "outputs": [],
   "source": [
    "response1 = \"\"\"\n",
    "Com base no artigo analisado, vou criar uma replicação focada em comparar métodos de **shallow learning** com os resultados de **deep learning** (ViT) apresentados no paper. O artigo propõe um framework de Federated Learning com Vision Transformers, mas nossa replicação será centralizada usando métodos tradicionais.\n",
    "\n",
    "## Estrutura da Replicação\n",
    "\n",
    "```python\n",
    "# requirements.txt\n",
    "numpy==1.24.3\n",
    "pandas==2.0.2\n",
    "scikit-learn==1.3.0\n",
    "opencv-python==4.8.0.74\n",
    "matplotlib==3.7.1\n",
    "seaborn==0.12.2\n",
    "pillow==10.0.0\n",
    "joblib==1.3.1\n",
    "tqdm==4.65.0\n",
    "```\n",
    "\n",
    "## 1. Configuração e Utilitários\n",
    "\n",
    "```python\n",
    "# utils.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class Config:\n",
    "    #Configurações globais do projeto\n",
    "    # Caminhos dos datasets\n",
    "    HAM10000_PATH = \"datasets/HAM10000\"\n",
    "    ISIC2019_PATH = \"datasets/ISIC\"\n",
    "    \n",
    "    # Parâmetros de processamento\n",
    "    IMG_SIZE = (224, 224)  # Mesmo tamanho usado no ViT\n",
    "    RANDOM_STATE = 42\n",
    "    TEST_SIZE = 0.2\n",
    "    CV_FOLDS = 5\n",
    "    \n",
    "    # Classes HAM10000 (7 classes como no paper)\n",
    "    HAM10000_CLASSES = {\n",
    "        'akiec': 'Actinic keratoses',\n",
    "        'bcc': 'Basal cell carcinoma', \n",
    "        'bkl': 'Benign keratosis-like lesions',\n",
    "        'df': 'Dermatofibroma',\n",
    "        'mel': 'Melanoma',\n",
    "        'nv': 'Melanocytic nevi',\n",
    "        'vasc': 'Vascular lesions'\n",
    "    }\n",
    "    \n",
    "    # Classes ISIC2019 (8 classes como no paper)\n",
    "    ISIC2019_CLASSES = {\n",
    "        'MEL': 'Melanoma',\n",
    "        'NV': 'Melanocytic nevus',\n",
    "        'BCC': 'Basal cell carcinoma',\n",
    "        'AK': 'Actinic keratosis',\n",
    "        'BKL': 'Benign keratosis',\n",
    "        'DF': 'Dermatofibroma',\n",
    "        'VASC': 'Vascular lesion',\n",
    "        'SCC': 'Squamous cell carcinoma'\n",
    "    }\n",
    "\n",
    "def load_image(image_path, target_size=(224, 224)):\n",
    "    #Carrega e pré-processa uma imagem\n",
    "    try:\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            return None\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, target_size)\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar imagem {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_color_features(image):\n",
    "    #Extrai características de cor da imagem\n",
    "    features = []\n",
    "    \n",
    "    # Estatísticas básicas por canal RGB\n",
    "    for channel in range(3):\n",
    "        channel_data = image[:, :, channel].flatten()\n",
    "        features.extend([\n",
    "            np.mean(channel_data),\n",
    "            np.std(channel_data),\n",
    "            np.median(channel_data),\n",
    "            np.percentile(channel_data, 25),\n",
    "            np.percentile(channel_data, 75)\n",
    "        ])\n",
    "    \n",
    "    # Histograma de cores (bins reduzidos para eficiência)\n",
    "    hist_r = cv2.calcHist([image], [0], None, [16], [0, 256])\n",
    "    hist_g = cv2.calcHist([image], [1], None, [16], [0, 256])\n",
    "    hist_b = cv2.calcHist([image], [2], None, [16], [0, 256])\n",
    "    \n",
    "    features.extend(hist_r.flatten())\n",
    "    features.extend(hist_g.flatten())\n",
    "    features.extend(hist_b.flatten())\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "def extract_texture_features(image):\n",
    "    # características de textura usando filtros simples\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    features = []\n",
    "    \n",
    "    # Gradientes\n",
    "    grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    \n",
    "    features.extend([\n",
    "        np.mean(np.abs(grad_x)),\n",
    "        np.std(grad_x),\n",
    "        np.mean(np.abs(grad_y)),\n",
    "        np.std(grad_y)\n",
    "    ])\n",
    "    \n",
    "    # Laplaciano (detecção de bordas)\n",
    "    laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n",
    "    features.extend([\n",
    "        np.mean(np.abs(laplacian)),\n",
    "        np.std(laplacian)\n",
    "    ])\n",
    "    \n",
    "    # Estatísticas básicas da imagem em escala de cinza\n",
    "    features.extend([\n",
    "        np.mean(gray),\n",
    "        np.std(gray),\n",
    "        np.median(gray),\n",
    "        np.percentile(gray, 25),\n",
    "        np.percentile(gray, 75)\n",
    "    ])\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "def extract_shape_features(image):\n",
    "    #Extrai características de forma simples\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Binarização simples\n",
    "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Contornos\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    features = []\n",
    "    if contours:\n",
    "        # Maior contorno\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        area = cv2.contourArea(largest_contour)\n",
    "        perimeter = cv2.arcLength(largest_contour, True)\n",
    "        \n",
    "        features.extend([\n",
    "            area,\n",
    "            perimeter,\n",
    "            area / (perimeter + 1e-7),  # Compacidade\n",
    "            len(largest_contour)  # Número de pontos do contorno\n",
    "        ])\n",
    "    else:\n",
    "        features.extend([0, 0, 0, 0])\n",
    "    \n",
    "    return np.array(features)\n",
    "```\n",
    "\n",
    "## 2. Carregamento e Processamento dos Datasets\n",
    "\n",
    "```python\n",
    "# data_loader.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils import Config, load_image, extract_color_features, extract_texture_features, extract_shape_features\n",
    "\n",
    "class DatasetLoader:\n",
    "    def __init__(self):\n",
    "        self.config = Config()\n",
    "    \n",
    "    def load_ham10000(self):\n",
    "        #Carrega o dataset HAM10000\n",
    "        print(\"Carregando dataset HAM10000...\")\n",
    "        \n",
    "        # Carrega metadados\n",
    "        metadata_path = os.path.join(self.config.HAM10000_PATH, \"HAM10000_metadata.csv\")\n",
    "        if not os.path.exists(metadata_path):\n",
    "            print(f\"Arquivo de metadados não encontrado: {metadata_path}\")\n",
    "            return None, None\n",
    "        \n",
    "        metadata = pd.read_csv(metadata_path)\n",
    "        \n",
    "        # Diretórios de imagens\n",
    "        img_dirs = [\n",
    "            os.path.join(self.config.HAM10000_PATH, \"HAM10000_images_part_1\"),\n",
    "            os.path.join(self.config.HAM10000_PATH, \"HAM10000_images_part_2\")\n",
    "        ]\n",
    "        \n",
    "        features_list = []\n",
    "        labels_list = []\n",
    "        \n",
    "        for idx, row in tqdm(metadata.iterrows(), total=len(metadata), desc=\"Processando HAM10000\"):\n",
    "            image_id = row['image_id']\n",
    "            label = row['dx']\n",
    "            \n",
    "            # Procura a imagem nos diretórios\n",
    "            image_path = None\n",
    "            for img_dir in img_dirs:\n",
    "                potential_path = os.path.join(img_dir, f\"{image_id}.jpg\")\n",
    "                if os.path.exists(potential_path):\n",
    "                    image_path = potential_path\n",
    "                    break\n",
    "            \n",
    "            if image_path is None:\n",
    "                continue\n",
    "            \n",
    "            # Carrega e processa a imagem\n",
    "            image = load_image(image_path, self.config.IMG_SIZE)\n",
    "            if image is None:\n",
    "                continue\n",
    "            \n",
    "            # Extrai características\n",
    "            color_feat = extract_color_features(image)\n",
    "            texture_feat = extract_texture_features(image)\n",
    "            shape_feat = extract_shape_features(image)\n",
    "            \n",
    "            # Combina todas as características\n",
    "            features = np.concatenate([color_feat, texture_feat, shape_feat])\n",
    "            \n",
    "            features_list.append(features)\n",
    "            labels_list.append(label)\n",
    "        \n",
    "        print(f\"HAM10000: {len(features_list)} imagens processadas\")\n",
    "        return np.array(features_list), np.array(labels_list)\n",
    "    \n",
    "    def load_isic2019(self):\n",
    "        #Carrega o dataset ISIC2019\n",
    "        print(\"Carregando dataset ISIC2019...\")\n",
    "        \n",
    "        # Carrega metadados\n",
    "        metadata_path = os.path.join(self.config.ISIC2019_PATH, \"ISIC_2019_Training_GroundTruth.csv\")\n",
    "        if not os.path.exists(metadata_path):\n",
    "            print(f\"Arquivo de metadados não encontrado: {metadata_path}\")\n",
    "            return None, None\n",
    "        \n",
    "        metadata = pd.read_csv(metadata_path)\n",
    "        \n",
    "        # Diretório de imagens\n",
    "        img_dir = os.path.join(self.config.ISIC2019_PATH, \"ISIC_2019_Training_Input\")\n",
    "        \n",
    "        features_list = []\n",
    "        labels_list = []\n",
    "        \n",
    "        for idx, row in tqdm(metadata.iterrows(), total=len(metadata), desc=\"Processando ISIC2019\"):\n",
    "            image_id = row['image']\n",
    "            \n",
    "            # Encontra a classe (coluna com valor 1)\n",
    "            class_cols = ['MEL', 'NV', 'BCC', 'AK', 'BKL', 'DF', 'VASC', 'SCC']\n",
    "            label = None\n",
    "            for col in class_cols:\n",
    "                if row[col] == 1:\n",
    "                    label = col\n",
    "                    break\n",
    "            \n",
    "            if label is None:\n",
    "                continue\n",
    "            \n",
    "            # Carrega imagem\n",
    "            image_path = os.path.join(img_dir, f\"{image_id}.jpg\")\n",
    "            if not os.path.exists(image_path):\n",
    "                continue\n",
    "            \n",
    "            image = load_image(image_path, self.config.IMG_SIZE)\n",
    "            if image is None:\n",
    "                continue\n",
    "            \n",
    "            # Extrai características\n",
    "            color_feat = extract_color_features(image)\n",
    "            texture_feat = extract_texture_features(image)\n",
    "            shape_feat = extract_shape_features(image)\n",
    "            \n",
    "            # Combina todas as características\n",
    "            features = np.concatenate([color_feat, texture_feat, shape_feat])\n",
    "            \n",
    "            features_list.append(features)\n",
    "            labels_list.append(label)\n",
    "        \n",
    "        print(f\"ISIC2019: {len(features_list)} imagens processadas\")\n",
    "        return np.array(features_list), np.array(labels_list)\n",
    "\n",
    "def prepare_data(features, labels):\n",
    "    #Prepara os dados para treinamento\n",
    "    # Codifica labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels_encoded = label_encoder.fit_transform(labels)\n",
    "    \n",
    "    # Normaliza características\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "    \n",
    "    return features_scaled, labels_encoded, label_encoder, scaler\n",
    "```\n",
    "\n",
    "## 3. Modelos de Shallow Learning\n",
    "\n",
    "```python\n",
    "# models.py\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "class ShallowModels:\n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.models = {}\n",
    "        self.best_models = {}\n",
    "        \n",
    "    def initialize_models(self):\n",
    "       #Inicializa os modelos com hiperparâmetros para busca\n",
    "        \n",
    "        # Random Forest\n",
    "        self.models['Random Forest'] = {\n",
    "            'model': RandomForestClassifier(random_state=self.random_state),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [10, 20, None],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Gradient Boosting\n",
    "        self.models['Gradient Boosting'] = {\n",
    "            'model': GradientBoostingClassifier(random_state=self.random_state),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200],\n",
    "                'learning_rate': [0.05, 0.1, 0.2],\n",
    "                'max_depth': [3, 5, 7]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Support Vector Machine\n",
    "        self.models['SVM'] = {\n",
    "            'model': SVC(random_state=self.random_state, probability=True),\n",
    "            'params': {\n",
    "                'C': [0.1, 1, 10, 100],\n",
    "                'kernel': ['rbf', 'poly'],\n",
    "                'gamma': ['scale', 'auto']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Logistic Regression\n",
    "        self.models['Logistic Regression'] = {\n",
    "            'model': LogisticRegression(random_state=self.random_state, max_iter=1000),\n",
    "            'params': {\n",
    "                'C': [0.1, 1, 10, 100],\n",
    "                'penalty': ['l1', 'l2'],\n",
    "                'solver': ['liblinear', 'saga']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # K-Nearest Neighbors\n",
    "        self.models['KNN'] = {\n",
    "            'model': KNeighborsClassifier(),\n",
    "            'params': {\n",
    "                'n_neighbors': [3, 5, 7, 9, 11],\n",
    "                'weights': ['uniform', 'distance'],\n",
    "                'metric': ['euclidean', 'manhattan']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Naive Bayes\n",
    "        self.models['Naive Bayes'] = {\n",
    "            'model': GaussianNB(),\n",
    "            'params': {\n",
    "                'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Decision Tree\n",
    "        self.models['Decision Tree'] = {\n",
    "            'model': DecisionTreeClassifier(random_state=self.random_state),\n",
    "            'params': {\n",
    "                'max_depth': [5, 10, 15, 20, None],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # AdaBoost\n",
    "        self.models['AdaBoost'] = {\n",
    "            'model': AdaBoostClassifier(random_state=self.random_state),\n",
    "            'params': {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'learning_rate': [0.5, 1.0, 1.5]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def train_with_grid_search(self, X_train, y_train, cv=5, n_jobs=-1):\n",
    "        #Treina modelos com busca em grade\n",
    "        print(\"Iniciando treinamento com Grid Search...\")\n",
    "        \n",
    "        for name, model_info in self.models.items():\n",
    "            print(f\"\\nTreinando {name}...\")\n",
    "            \n",
    "            # Grid Search\n",
    "            grid_search = GridSearchCV(\n",
    "                model_info['model'],\n",
    "                model_info['params'],\n",
    "                cv=cv,\n",
    "                scoring='accuracy',\n",
    "                n_jobs=n_jobs,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            grid_search.fit(X_train, y_train)\n",
    "            \n",
    "            # Armazena o melhor modelo\n",
    "            self.best_models[name] = {\n",
    "                'model': grid_search.best_estimator_,\n",
    "                'best_params': grid_search.best_params_,\n",
    "                'best_score': grid_search.best_score_\n",
    "            }\n",
    "            \n",
    "            print(f\"{name} - Melhor score CV: {grid_search.best_score_:.4f}\")\n",
    "            print(f\"{name} - Melhores parâmetros: {grid_search.best_params_}\")\n",
    "    \n",
    "    def get_best_models(self):\n",
    "        #Retorna os melhores modelos treinados\n",
    "        return self.best_models\n",
    "```\n",
    "\n",
    "## 4. Avaliação e Métricas\n",
    "\n",
    "```python\n",
    "# evaluation.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                           f1_score, confusion_matrix, classification_report, \n",
    "                           roc_auc_score, roc_curve, auc)\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import joblib\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "        \n",
    "    def evaluate_model(self, model, model_name, X_test, y_test, class_names=None):\n",
    "        #Avalia um modelo individual\n",
    "        \n",
    "        # Predições\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = None\n",
    "        \n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_pred_proba = model.predict_proba(X_test)\n",
    "        \n",
    "        # Métricas básicas\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "        \n",
    "        # AUC (para multiclasse)\n",
    "        auc_score = None\n",
    "        if y_pred_proba is not None:\n",
    "            try:\n",
    "                # Binariza as labels para cálculo do AUC\n",
    "                n_classes = len(np.unique(y_test))\n",
    "                y_test_bin = label_binarize(y_test, classes=range(n_classes))\n",
    "                \n",
    "                if n_classes == 2:\n",
    "                    auc_score = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "                else:\n",
    "                    auc_score = roc_auc_score(y_test_bin, y_pred_proba, \n",
    "                                            average='macro', multi_class='ovr')\n",
    "            except:\n",
    "                auc_score = None\n",
    "        \n",
    "        # Armazena resultados\n",
    "        self.results[model_name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'auc': auc_score,\n",
    "            'y_pred': y_pred,\n",
    "            'y_pred_proba': y_pred_proba,\n",
    "            'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        return self.results[model_name]\n",
    "    \n",
    "    def evaluate_all_models(self, models_dict, X_test, y_test, class_names=None):\n",
    "        #Avalia todos os modelos\n",
    "        print(\"Avaliando modelos...\")\n",
    "        \n",
    "        for name, model_info in models_dict.items():\n",
    "            model = model_info['model']\n",
    "            print(f\"Avaliando {name}...\")\n",
    "            self.evaluate_model(model, name, X_test, y_test, class_names)\n",
    "    \n",
    "    def print_results_summary(self):\n",
    "        #Imprime resumo dos resultados\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"RESUMO DOS RESULTADOS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Cria DataFrame com resultados\n",
    "        results_data = []\n",
    "        for model_name, metrics in self.results.items():\n",
    "            results_data.append({\n",
    "                'Modelo': model_name,\n",
    "                'Acurácia': f\"{metrics['accuracy']:.4f}\",\n",
    "                'Precisão': f\"{metrics['precision']:.4f}\",\n",
    "                'Recall': f\"{metrics['recall']:.4f}\",\n",
    "                'F1-Score': f\"{metrics['f1_score']:.4f}\",\n",
    "                'AUC': f\"{metrics['auc']:.4f}\" if metrics['auc'] else \"N/A\"\n",
    "            })\n",
    "        \n",
    "        df_results = pd.DataFrame(results_data)\n",
    "        print(df_results.to_string(index=False))\n",
    "        \n",
    "        # Melhor modelo por métrica\n",
    "        print(f\"\\nMelhor Acurácia: {max(self.results.keys(), key=lambda x: self.results[x]['accuracy'])}\")\n",
    "        print(f\"Melhor F1-Score: {max(self.results.keys(), key=lambda x: self.results[x]['f1_score'])}\")\n",
    "        \n",
    "        if any(self.results[x]['auc'] for x in self.results if self.results[x]['auc']):\n",
    "            best_auc = max([x for x in self.results.keys() if self.results[x]['auc']], \n",
    "                          key=lambda x: self.results[x]['auc'])\n",
    "            print(f\"Melhor AUC: {best_auc}\")\n",
    "    \n",
    "    def plot_results(self, dataset_name=\"Dataset\", save_path=None):\n",
    "        #Plota gráficos comparativos\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle(f'Comparação de Modelos - {dataset_name}', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Prepara dados para plotagem\n",
    "        models = list(self.results.keys())\n",
    "        metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "        metric_names = ['Acurácia', 'Precisão', 'Recall', 'F1-Score']\n",
    "        \n",
    "        for idx, (metric, metric_name) in enumerate(zip(metrics, metric_names)):\n",
    "            ax = axes[idx//2, idx%2]\n",
    "            values = [self.results[model][metric] for model in models]\n",
    "            \n",
    "            bars = ax.bar(models, values, alpha=0.7, color=plt.cm.Set3(np.linspace(0, 1, len(models))))\n",
    "            ax.set_title(metric_name, fontweight='bold')\n",
    "            ax.set_ylabel('Score')\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Adiciona valores nas barras\n",
    "            for bar, value in zip(bars, values):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                       f'{value:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_confusion_matrices(self, class_names=None, dataset_name=\"Dataset\", save_path=None):\n",
    "        #Plota matrizes de confusão para todos os modelos\n",
    "        n_models = len(self.results)\n",
    "        cols = 3\n",
    "        rows = (n_models + cols - 1) // cols\n",
    "        \n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
    "        fig.suptitle(f'Matrizes de Confusão - {dataset_name}', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        if n_models == 1:\n",
    "            axes = [axes]\n",
    "        elif rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for idx, (model_name, results) in enumerate(self.results.items()):\n",
    "            row, col = idx // cols, idx % cols\n",
    "            ax = axes[row, col] if rows > 1 else axes[col]\n",
    "            \n",
    "            cm = results['confusion_matrix']\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                       xticklabels=class_names, yticklabels=class_names)\n",
    "            ax.set_title(f'{model_name}\\nAcc: {results[\"accuracy\"]:.3f}')\n",
    "            ax.set_xlabel('Predito')\n",
    "            ax.set_ylabel('Real')\n",
    "        \n",
    "        # Remove subplots vazios\n",
    "        for idx in range(n_models, rows * cols):\n",
    "            row, col = idx // cols, idx % cols\n",
    "            ax = axes[row, col] if rows > 1 else axes[col]\n",
    "            ax.remove()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def compare_with_paper_results(self, paper_results):\n",
    "        #Compara resultados com os do paper\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"COMPARAÇÃO COM RESULTADOS DO PAPER\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"Resultados do Paper (ViT Federado):\")\n",
    "        for metric, value in paper_results.items():\n",
    "            print(f\"  {metric}: {value}\")\n",
    "        \n",
    "        print(f\"\\nMelhores Resultados dos Modelos Shallow:\")\n",
    "        best_acc = max(self.results.keys(), key=lambda x: self.results[x]['accuracy'])\n",
    "        best_f1 = max(self.results.keys(), key=lambda x: self.results[x]['f1_score'])\n",
    "        \n",
    "        print(f\"  Melhor Acurácia: {best_acc} ({self.results[best_acc]['accuracy']:.4f})\")\n",
    "        print(f\"  Melhor F1-Score: {best_f1} ({self.results[best_f1]['f1_score']:.4f})\")\n",
    "        \n",
    "        if 'auc' in paper_results:\n",
    "            best_auc_models = [x for x in self.results.keys() if self.results[x]['auc']]\n",
    "            if best_auc_models:\n",
    "                best_auc = max(best_auc_models, key=lambda x: self.results[x]['auc'])\n",
    "                print(f\"  Melhor AUC: {best_auc} ({self.results[best_auc]['auc']:.4f})\")\n",
    "\n",
    "    def save_results(self, filepath):\n",
    "        #Salva resultados em arquivo\n",
    "        joblib.dump(self.results, filepath)\n",
    "        print(f\"Resultados salvos em: {filepath}\")\n",
    "    \n",
    "    def load_results(self, filepath):\n",
    "       #Carrega resultados de arquivo\n",
    "        self.results = joblib.load(filepath)\n",
    "        print(f\"Resultados carregados de: {filepath}\")\n",
    "```\n",
    "\n",
    "## 5. Script Principal\n",
    "\n",
    "```python\n",
    "# main.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import Config\n",
    "from data_loader import DatasetLoader, prepare_data\n",
    "from models import ShallowModels\n",
    "from evaluation import ModelEvaluator\n",
    "\n",
    "def main():\n",
    "    # Configuração\n",
    "    config = Config()\n",
    "    \n",
    "    # Resultados do paper para comparação\n",
    "    paper_results_ham10000 = {\n",
    "        'Global Test Accuracy': '90%',\n",
    "        'Sensitivity': '88.2%',\n",
    "        'Specificity': '91.4%',\n",
    "        'Best AUC': '0.96'\n",
    "    }\n",
    "    \n",
    "    paper_results_isic2019 = {\n",
    "        'Global Test Accuracy': '87.6%',\n",
    "        'Best AUC': '0.96'\n",
    "    }\n",
    "    \n",
    "    # Carregador de dados\n",
    "    loader = DatasetLoader()\n",
    "    \n",
    "    # Processa HAM10000\n",
    "    print(\"=\"*80)\n",
    "    print(\"PROCESSANDO DATASET HAM10000\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    X_ham, y_ham = loader.load_ham10000()\n",
    "    \n",
    "    if X_ham is not None and y_ham is not None:\n",
    "        # Prepara dados\n",
    "        X_ham_scaled, y_ham_encoded, label_encoder_ham, scaler_ham = prepare_data(X_ham, y_ham)\n",
    "        \n",
    "        # Split treino/teste\n",
    "        X_train_ham, X_test_ham, y_train_ham, y_test_ham = train_test_split(\n",
    "            X_ham_scaled, y_ham_encoded, \n",
    "            test_size=config.TEST_SIZE, \n",
    "            random_state=config.RANDOM_STATE,\n",
    "            stratify=y_ham_encoded\n",
    "        )\n",
    "        \n",
    "        print(f\"HAM10000 - Treino: {X_train_ham.shape[0]}, Teste: {X_test_ham.shape[0]}\")\n",
    "        print(f\"Número de características extraídas: {X_ham_scaled.shape[1]}\")\n",
    "        print(f\"Classes: {list(label_encoder_ham.classes_)}\")\n",
    "        \n",
    "        # Treina modelos\n",
    "        models_ham = ShallowModels(random_state=config.RANDOM_STATE)\n",
    "        models_ham.initialize_models()\n",
    "        models_ham.train_with_grid_search(X_train_ham, y_train_ham, cv=config.CV_FOLDS)\n",
    "        \n",
    "        # Avalia modelos\n",
    "        evaluator_ham = ModelEvaluator()\n",
    "        evaluator_ham.evaluate_all_models(\n",
    "            models_ham.get_best_models(), \n",
    "            X_test_ham, \n",
    "            y_test_ham,\n",
    "            class_names=label_encoder_ham.classes_\n",
    "        )\n",
    "        \n",
    "        # Resultados\n",
    "        evaluator_ham.print_results_summary()\n",
    "        evaluator_ham.plot_results(\"HAM10000\", \"results_ham10000_comparison.png\")\n",
    "        evaluator_ham.plot_confusion_matrices(\n",
    "            label_encoder_ham.classes_, \n",
    "            \"HAM10000\", \n",
    "            \"confusion_matrices_ham10000.png\"\n",
    "        )\n",
    "        evaluator_ham.compare_with_paper_results(paper_results_ham10000)\n",
    "        evaluator_ham.save_results(\"results_ham10000.pkl\")\n",
    "        \n",
    "        # Salva modelos e preprocessors\n",
    "        joblib.dump(models_ham.get_best_models(), \"models_ham10000.pkl\")\n",
    "        joblib.dump(label_encoder_ham, \"label_encoder_ham10000.pkl\")\n",
    "        joblib.dump(scaler_ham, \"scaler_ham10000.pkl\")\n",
    "    \n",
    "    # Processa ISIC2019\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PROCESSANDO DATASET ISIC2019\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    X_isic, y_isic = loader.load_isic2019()\n",
    "    \n",
    "    if X_isic is not None and y_isic is not None:\n",
    "        # Prepara dados\n",
    "        X_isic_scaled, y_isic_encoded, label_encoder_isic, scaler_isic = prepare_data(X_isic, y_isic)\n",
    "        \n",
    "        # Split treino/teste\n",
    "        X_train_isic, X_test_isic, y_train_isic, y_test_isic = train_test_split(\n",
    "            X_isic_scaled, y_isic_encoded,\n",
    "            test_size=config.TEST_SIZE,\n",
    "            random_state=config.RANDOM_STATE,\n",
    "            stratify=y_isic_encoded\n",
    "        )\n",
    "        \n",
    "        print(f\"ISIC2019 - Treino: {X_train_isic.shape[0]}, Teste: {X_test_isic.shape[0]}\")\n",
    "        print(f\"Número de características extraídas: {X_isic_scaled.shape[1]}\")\n",
    "        print(f\"Classes: {list(label_encoder_isic.classes_)}\")\n",
    "        \n",
    "        # Treina modelos\n",
    "        models_isic = ShallowModels(random_state=config.RANDOM_STATE)\n",
    "        models_isic.initialize_models()\n",
    "        models_isic.train_with_grid_search(X_train_isic, y_train_isic, cv=config.CV_FOLDS)\n",
    "        \n",
    "        # Avalia modelos\n",
    "        evaluator_isic = ModelEvaluator()\n",
    "        evaluator_isic.evaluate_all_models(\n",
    "            models_isic.get_best_models(),\n",
    "            X_test_isic,\n",
    "            y_test_isic,\n",
    "            class_names=label_encoder_isic.classes_\n",
    "        )\n",
    "        \n",
    "        # Resultados\n",
    "        evaluator_isic.print_results_summary()\n",
    "        evaluator_isic.plot_results(\"ISIC2019\", \"results_isic2019_comparison.png\")\n",
    "        evaluator_isic.plot_confusion_matrices(\n",
    "            label_encoder_isic.classes_,\n",
    "            \"ISIC2019\",\n",
    "            \"confusion_matrices_isic2019.png\"\n",
    "        )\n",
    "        evaluator_isic.compare_with_paper_results(paper_results_isic2019)\n",
    "        evaluator_isic.save_results(\"results_isic2019.pkl\")\n",
    "        \n",
    "        # Salva modelos e preprocessors\n",
    "        joblib.dump(models_isic.get_best_models(), \"models_isic2019.pkl\")\n",
    "        joblib.dump(label_encoder_isic, \"label_encoder_isic2019.pkl\")\n",
    "        joblib.dump(scaler_isic, \"scaler_isic2019.pkl\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "## 6. Script de Análise Detalhada\n",
    "\n",
    "```python\n",
    "# detailed_analysis.py\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def analyze_results():\n",
    "    #Análise detalhada dos resultados\n",
    "    \n",
    "    # Carrega resultados\n",
    "    try:\n",
    "        results_ham = joblib.load(\"results_ham10000.pkl\")\n",
    "        results_isic = joblib.load(\"results_isic2019.pkl\")\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"ANÁLISE DETALHADA DOS RESULTADOS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Análise HAM10000\n",
    "        print(\"\\nHAM10000 - Análise por Classe:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Carrega label encoder para nomes das classes\n",
    "        label_encoder_ham = joblib.load(\"label_encoder_ham10000.pkl\")\n",
    "        \n",
    "        for model_name, metrics in results_ham.items():\n",
    "            print(f\"\\n{model_name}:\")\n",
    "            # Aqui você pode adicionar análises mais detalhadas por classe\n",
    "            print(f\"  Acurácia Global: {metrics['accuracy']:.4f}\")\n",
    "            print(f\"  F1-Score Macro: {metrics['f1_score']:.4f}\")\n",
    "            if metrics['auc']:\n",
    "                print(f\"  AUC: {metrics['auc']:.4f}\")\n",
    "        \n",
    "        # Análise ISIC2019\n",
    "        print(\"\\n\\nISIC2019 - Análise por Classe:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        label_encoder_isic = joblib.load(\"label_encoder_isic2019.pkl\")\n",
    "        \n",
    "        for model_name, metrics in results_isic.items():\n",
    "            print(f\"\\n{model_name}:\")\n",
    "            print(f\"  Acurácia Global: {metrics['accuracy']:.4f}\")\n",
    "            print(f\"  F1-Score Macro: {metrics['f1_score']:.4f}\")\n",
    "            if metrics['auc']:\n",
    "                print(f\"  AUC: {metrics['auc']:.4f}\")\n",
    "        \n",
    "        # Comparação final\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"COMPARAÇÃO FINAL: SHALLOW vs DEEP LEARNING\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(\"\\nHAM10000:\")\n",
    "        print(\"  Paper (ViT Federado): 90% acurácia, 88.2% sensibilidade, 91.4% especificidade\")\n",
    "        best_ham = max(results_ham.keys(), key=lambda x: results_ham[x]['accuracy'])\n",
    "        print(f\"  Melhor Shallow ({best_ham}): {results_ham[best_ham]['accuracy']:.1%} acurácia\")\n",
    "        \n",
    "        print(\"\\nISIC2019:\")\n",
    "        print(\"  Paper (ViT Federado): 87.6% acurácia\")\n",
    "        best_isic = max(results_isic.keys(), key=lambda x: results_isic[x]['accuracy'])\n",
    "        print(f\"  Melhor Shallow ({best_isic}): {results_isic[best_isic]['accuracy']:.1%} acurácia\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Arquivo não encontrado: {e}\")\n",
    "        print(\"Execute o script main.py primeiro para gerar os resultados.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_results()\n",
    "```\n",
    "\n",
    "## 7. README.md\n",
    "\n",
    "```markdown\n",
    "# Replicação: Federated ViT vs Shallow Learning para Classificação de Câncer de Pele\n",
    "\n",
    "Este projeto replica e compara os resultados do artigo \"Federated ViT: A Distributed Deep Learning Framework for Skin Cancer Classification\" usando métodos de **shallow learning** ao invés de deep learning.\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Comparar o desempenho de métodos tradicionais de machine learning (shallow learning) com os resultados de Vision Transformers federados apresentados no paper original.\n",
    "\n",
    "## Datasets\n",
    "\n",
    "- **HAM10000**: 10.015 imagens, 7 classes de lesões de pele\n",
    "- **ISIC2019**: 25.331 imagens, 8 classes de lesões de pele\n",
    "\n",
    "## Metodologia\n",
    "\n",
    "### Extração de Características\n",
    "- **Características de Cor**: Estatísticas RGB, histogramas\n",
    "- **Características de Textura**: Gradientes, Laplaciano, estatísticas\n",
    "- **Características de Forma**: Área, perímetro, compacidade\n",
    "\n",
    "### Modelos Testados\n",
    "1. Random Forest\n",
    "2. Gradient Boosting\n",
    "3. Support Vector Machine (SVM)\n",
    "4. Logistic Regression\n",
    "5. K-Nearest Neighbors (KNN)\n",
    "6. Naive Bayes\n",
    "7. Decision Tree\n",
    "8. AdaBoost\n",
    "\n",
    "## Execução\n",
    "\n",
    "```bash\n",
    "# Instalar dependências\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Executar experimento principal\n",
    "python main.py\n",
    "\n",
    "# Análise detalhada dos resultados\n",
    "python detailed_analysis.py\n",
    "```\n",
    "\n",
    "## Estrutura dos Arquivos\n",
    "\n",
    "```\n",
    "├── utils.py              # Utilitários e configurações\n",
    "├── data_loader.py        # Carregamento e processamento dos dados\n",
    "├── models.py             # Modelos de shallow learning\n",
    "├── evaluation.py         # Avaliação e métricas\n",
    "├── main.py               # Script principal\n",
    "├── detailed_analysis.py  # Análise detalhada\n",
    "└── requirements.txt      # Dependências\n",
    "```\n",
    "\n",
    "## Resultados Esperados\n",
    "\n",
    "O projeto gerará:\n",
    "- Comparação de performance entre modelos shallow\n",
    "- Gráficos comparativos de métricas\n",
    "- Matrizes de confusão\n",
    "- Comparação com resultados do paper original\n",
    "- Análise detalhada por classe\n",
    "\n",
    "## Comparação com o Paper Original\n",
    "\n",
    "**Paper (ViT Federado)**:\n",
    "- HAM10000: 90% acurácia global\n",
    "- ISIC2019: 87.6% acurácia global\n",
    "\n",
    "**Este estudo (Shallow Learning)**:\n",
    "- Resultados serão comparados após execução\n",
    "```\n",
    "\n",
    "## Como Executar\n",
    "\n",
    "1. **Prepare os dados**: Certifique-se de que os datasets estão na estrutura correta\n",
    "2. **Instale as dependências**: `pip install -r requirements.txt`\n",
    "3. **Execute o experimento**: `python main.py`\n",
    "4. **Analise os resultados**: `python detailed_analysis.py`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb687eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = \"\"\"\n",
    "Quanto tempo você estima para finalizar tudo, estou usando uma máquina na aws com essas especificações:\n",
    "´´´\n",
    "Amazon Linux 2023 (kernel-6.1) is a modern, general purpose Linux-based OS that comes with 5 years of long term support. It is optimized for AWS and designed to provide a secure, stable and high-performance execution environment to develop and run your cloud applications.\n",
    "\n",
    "Amazon Linux 2023 AMI 2023.9.20251014.0 x86_64 HVM kernel-6.1\n",
    "t2.2xlarge\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "551ddfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_rules = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "    {\"role\": \"assistant\", \"content\": response1},\n",
    "    {\"role\": \"user\", \"content\": prompt2}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a21a590c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para uma instância **t2.2xlarge** (8 vCPUs, 32GB RAM), aqui está minha estimativa de tempo:\n",
      "\n",
      "## Estimativa de Tempo Total: **3-6 horas**\n",
      "\n",
      "### Breakdown detalhado:\n",
      "\n",
      "#### 1. **Preparação do ambiente** (~15-30 min)\n",
      "```bash\n",
      "# Instalação de dependências\n",
      "sudo yum update -y\n",
      "sudo yum install python3-pip -y\n",
      "pip3 install -r requirements.txt\n",
      "```\n",
      "\n",
      "#### 2. **Processamento dos datasets** (~1-2 horas)\n",
      "- **HAM10000** (~10.000 imagens): 30-45 min\n",
      "- **ISIC2019** (~25.000 imagens): 45-90 min\n",
      "- Extração de características (cor, textura, forma)\n",
      "- I/O intensivo, mas t2.2xlarge tem boa performance\n",
      "\n",
      "#### 3. **Treinamento dos modelos** (~1.5-3 horas)\n",
      "- **8 algoritmos** com **Grid Search CV (5-fold)**\n",
      "- **Por dataset**: 45-90 min cada\n",
      "- Modelos mais demorados:\n",
      "  - SVM com kernel RBF: ~30-45 min\n",
      "  - Random Forest: ~15-20 min\n",
      "  - Gradient Boosting: ~20-30 min\n",
      "\n",
      "#### 4. **Avaliação e visualização** (~30 min)\n",
      "- Métricas, gráficos, matrizes de confusão\n",
      "\n",
      "## Otimizações para acelerar:\n",
      "\n",
      "### 1. **Reduzir Grid Search** (economiza ~40-50% do tempo):\n",
      "```python\n",
      "# Em models.py, use menos parâmetros:\n",
      "'Random Forest': {\n",
      "    'params': {\n",
      "        'n_estimators': [100, 200],  # ao invés de [100, 200, 300]\n",
      "        'max_depth': [10, None],     # ao invés de [10, 20, None]\n",
      "        'min_samples_split': [2, 5]  # ao invés de [2, 5, 10]\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "### 2. **Paralelização máxima**:\n",
      "```python\n",
      "# Use todos os cores disponíveis\n",
      "grid_search = GridSearchCV(\n",
      "    model_info['model'],\n",
      "    model_info['params'],\n",
      "    cv=3,  # Reduzir de 5 para 3 folds\n",
      "    n_jobs=8,  # Usar todos os 8 vCPUs\n",
      "    verbose=1\n",
      ")\n",
      "```\n",
      "\n",
      "### 3. **Amostragem estratificada** (para testes rápidos):\n",
      "```python\n",
      "# Para teste inicial, use uma amostra menor\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Usar apenas 30% dos dados para teste rápido\n",
      "X_sample, _, y_sample, _ = train_test_split(\n",
      "    X_ham_scaled, y_ham_encoded, \n",
      "    train_size=0.3, \n",
      "    stratify=y_ham_encoded,\n",
      "    random_state=42\n",
      ")\n",
      "```\n",
      "\n",
      "## Script de monitoramento do progresso:\n",
      "\n",
      "```python\n",
      "# progress_monitor.py\n",
      "import time\n",
      "import psutil\n",
      "import logging\n",
      "\n",
      "logging.basicConfig(level=logging.INFO)\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "def monitor_progress():\n",
      "    \"\"\"Monitora uso de CPU/RAM durante execução\"\"\"\n",
      "    start_time = time.time()\n",
      "    \n",
      "    while True:\n",
      "        cpu_percent = psutil.cpu_percent(interval=1)\n",
      "        memory = psutil.virtual_memory()\n",
      "        \n",
      "        elapsed = time.time() - start_time\n",
      "        logger.info(f\"Tempo: {elapsed/60:.1f}min | CPU: {cpu_percent:.1f}% | RAM: {memory.percent:.1f}%\")\n",
      "        \n",
      "        time.sleep(30)  # Log a cada 30 segundos\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    monitor_progress()\n",
      "```\n",
      "\n",
      "## Execução otimizada:\n",
      "\n",
      "```bash\n",
      "# Terminal 1: Monitor de progresso\n",
      "python3 progress_monitor.py &\n",
      "\n",
      "# Terminal 2: Execução principal\n",
      "time python3 main.py\n",
      "```\n",
      "\n",
      "## Cenários de tempo:\n",
      "\n",
      "- **Execução completa**: 3-6 horas\n",
      "- **Com otimizações**: 2-4 horas  \n",
      "- **Amostra de teste (30% dados)**: 1-2 horas\n",
      "\n",
      "## Recomendações:\n",
      "\n",
      "1. **Comece com amostra pequena** para validar o código\n",
      "2. **Execute durante a madrugada** para aproveitar melhor os recursos\n",
      "3. **Use screen/tmux** para manter a sessão ativa:\n",
      "```bash\n",
      "screen -S skin_cancer_experiment\n",
      "python3 main.py\n",
      "# Ctrl+A+D para detach\n",
      "```\n",
      "\n",
      "4. **Monitore o storage**: Os datasets + resultados ocuparão ~15-20GB\n",
      "\n",
      "A instância t2.2xlarge é adequada para este experimento, mas considere usar uma **c5.2xlarge** se precisar de mais performance de CPU para o Grid Search.Para uma instância **t2.2xlarge** (8 vCPUs, 32GB RAM), aqui está minha estimativa de tempo:\n",
      "\n",
      "## Estimativa de Tempo Total: **3-6 horas**\n",
      "\n",
      "### Breakdown detalhado:\n",
      "\n",
      "#### 1. **Preparação do ambiente** (~15-30 min)\n",
      "```bash\n",
      "# Instalação de dependências\n",
      "sudo yum update -y\n",
      "sudo yum install python3-pip -y\n",
      "pip3 install -r requirements.txt\n",
      "```\n",
      "\n",
      "#### 2. **Processamento dos datasets** (~1-2 horas)\n",
      "- **HAM10000** (~10.000 imagens): 30-45 min\n",
      "- **ISIC2019** (~25.000 imagens): 45-90 min\n",
      "- Extração de características (cor, textura, forma)\n",
      "- I/O intensivo, mas t2.2xlarge tem boa performance\n",
      "\n",
      "#### 3. **Treinamento dos modelos** (~1.5-3 horas)\n",
      "- **8 algoritmos** com **Grid Search CV (5-fold)**\n",
      "- **Por dataset**: 45-90 min cada\n",
      "- Modelos mais demorados:\n",
      "  - SVM com kernel RBF: ~30-45 min\n",
      "  - Random Forest: ~15-20 min\n",
      "  - Gradient Boosting: ~20-30 min\n",
      "\n",
      "#### 4. **Avaliação e visualização** (~30 min)\n",
      "- Métricas, gráficos, matrizes de confusão\n",
      "\n",
      "## Otimizações para acelerar:\n",
      "\n",
      "### 1. **Reduzir Grid Search** (economiza ~40-50% do tempo):\n",
      "```python\n",
      "# Em models.py, use menos parâmetros:\n",
      "'Random Forest': {\n",
      "    'params': {\n",
      "        'n_estimators': [100, 200],  # ao invés de [100, 200, 300]\n",
      "        'max_depth': [10, None],     # ao invés de [10, 20, None]\n",
      "        'min_samples_split': [2, 5]  # ao invés de [2, 5, 10]\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "### 2. **Paralelização máxima**:\n",
      "```python\n",
      "# Use todos os cores disponíveis\n",
      "grid_search = GridSearchCV(\n",
      "    model_info['model'],\n",
      "    model_info['params'],\n",
      "    cv=3,  # Reduzir de 5 para 3 folds\n",
      "    n_jobs=8,  # Usar todos os 8 vCPUs\n",
      "    verbose=1\n",
      ")\n",
      "```\n",
      "\n",
      "### 3. **Amostragem estratificada** (para testes rápidos):\n",
      "```python\n",
      "# Para teste inicial, use uma amostra menor\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# Usar apenas 30% dos dados para teste rápido\n",
      "X_sample, _, y_sample, _ = train_test_split(\n",
      "    X_ham_scaled, y_ham_encoded, \n",
      "    train_size=0.3, \n",
      "    stratify=y_ham_encoded,\n",
      "    random_state=42\n",
      ")\n",
      "```\n",
      "\n",
      "## Script de monitoramento do progresso:\n",
      "\n",
      "```python\n",
      "# progress_monitor.py\n",
      "import time\n",
      "import psutil\n",
      "import logging\n",
      "\n",
      "logging.basicConfig(level=logging.INFO)\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "def monitor_progress():\n",
      "    \"\"\"Monitora uso de CPU/RAM durante execução\"\"\"\n",
      "    start_time = time.time()\n",
      "    \n",
      "    while True:\n",
      "        cpu_percent = psutil.cpu_percent(interval=1)\n",
      "        memory = psutil.virtual_memory()\n",
      "        \n",
      "        elapsed = time.time() - start_time\n",
      "        logger.info(f\"Tempo: {elapsed/60:.1f}min | CPU: {cpu_percent:.1f}% | RAM: {memory.percent:.1f}%\")\n",
      "        \n",
      "        time.sleep(30)  # Log a cada 30 segundos\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    monitor_progress()\n",
      "```\n",
      "\n",
      "## Execução otimizada:\n",
      "\n",
      "```bash\n",
      "# Terminal 1: Monitor de progresso\n",
      "python3 progress_monitor.py &\n",
      "\n",
      "# Terminal 2: Execução principal\n",
      "time python3 main.py\n",
      "```\n",
      "\n",
      "## Cenários de tempo:\n",
      "\n",
      "- **Execução completa**: 3-6 horas\n",
      "- **Com otimizações**: 2-4 horas  \n",
      "- **Amostra de teste (30% dados)**: 1-2 horas\n",
      "\n",
      "## Recomendações:\n",
      "\n",
      "1. **Comece com amostra pequena** para validar o código\n",
      "2. **Execute durante a madrugada** para aproveitar melhor os recursos\n",
      "3. **Use screen/tmux** para manter a sessão ativa:\n",
      "```bash\n",
      "screen -S skin_cancer_experiment\n",
      "python3 main.py\n",
      "# Ctrl+A+D para detach\n",
      "```\n",
      "\n",
      "4. **Monitore o storage**: Os datasets + resultados ocuparão ~15-20GB\n",
      "\n",
      "A instância t2.2xlarge é adequada para este experimento, mas considere usar uma **c5.2xlarge** se precisar de mais performance de CPU para o Grid Search.\n"
     ]
    }
   ],
   "source": [
    "response2 = run_claude_stream(zero_rules, system_prompt) \n",
    "print(response2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
